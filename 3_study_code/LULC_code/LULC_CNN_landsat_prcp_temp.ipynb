{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変数定義\n",
    "\n",
    "4 seasons * 7 bandsのデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define vars ---------------------------------------------\n",
    "under = 90\n",
    "gpu_list = ['/gpu:0', '/gpu:1']\n",
    "SEED = 31\n",
    "DEM = True\n",
    "\n",
    "# 共通params ---------------------------------------------------\n",
    "n_trials  = 2**5          # ベイズ最適化回数\n",
    "outer_cvs = 10\n",
    "inner_cvs = 10\n",
    "\n",
    "# CNN Training params ------------------------------------------\n",
    "train_epochs = 2**5      # エポック数\n",
    "best_epochs = 2**5       # 最終モデル決定用のエポック数\n",
    "early_stopping = 2**3    # \n",
    "\n",
    "# LightGBM params -----------------------------------------------\n",
    "lgb_boosting_type = 'gbdt'\n",
    "\n",
    "import os, zipfile, io, re\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# Utils -----------------------\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import ipynb_path\n",
    "from math import sqrt\n",
    "import tifffile\n",
    "# Machine Learning ---------------\n",
    "import lightgbm as LGB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from optuna import integration\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "# Keras, TensorFlow ---------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization, Convolution2D, Input\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 31\n",
    "np.random.seed(SEED)\n",
    "gpus = len(gpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     5,
     10,
     14,
     60,
     78,
     127,
     181,
     182,
     227,
     245,
     293,
     345,
     359,
     411,
     429,
     455,
     468,
     481,
     496,
     502,
     514,
     540,
     587,
     602
    ]
   },
   "outputs": [],
   "source": [
    "# define functions\n",
    "def pkl_saver(object, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as web:\n",
    "        pickle.dump(object , web)\n",
    "\n",
    "def pkl_loader(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as web:\n",
    "        data = pickle.load(web)\n",
    "    return data\n",
    "\n",
    "def dir_generator(dir_path):\n",
    "    if os.path.exists(dir_path) == False:\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "def train_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}.pkl')\n",
    "    else:\n",
    "        trial = int(len(imgfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "            Y_28 = file_split[5].split(\".\")[0]\n",
    "            point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "            filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "            X.append(X_28[box*28: (box+1)*28])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}.pkl'))\n",
    "        \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "def train_transform():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl'):\n",
    "        df_train = pkl_loader(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl')\n",
    "        print(\"train_df をインポートしたよ！\")\n",
    "    else:\n",
    "        print(\"pklを新しく作成中ですが，標準化・正規化のコードに書き直しましたか？\")\n",
    "        X_train_zeros = np.zeros((X_train.shape[0], N*N*28))\n",
    "        for i in range(len(X_train)):\n",
    "            for k in range(N):\n",
    "                for l in range(N):\n",
    "                    for j in range(28):\n",
    "                        X_train_zeros[i][j+k+l] = X_train[i][k][l][j]\n",
    "        # X_train の名前が重複していたので違う名前に変えた. X_train_tmp\n",
    "        X_train_tmp = X_train_zeros\n",
    "        df_train = pd.concat([make_df(Y_train), make_df(X_train_tmp)], axis=1)\n",
    "        pkl_saver(df_train, os.path.join(data_path, f'df_{N}x{N}_{standarization[standarization_num]}.pkl'))\n",
    "        \n",
    "    return df_train.iloc[:, 1:], make_df(Y_train)\n",
    "def train_dem_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_dem.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_dem.pkl')\n",
    "    else:\n",
    "        trial = int(len(imgfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)[0][0]\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "            dem_data = tifffile.imread(train_dem_files[box])[0][0]\n",
    "            X_28.append(dem_data)\n",
    "            Y_28 = file_split[5].split(\".\")[0]\n",
    "            point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "            filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "            X.append(X_28[box*29: (box+1)*29])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "    #         if box==100:break\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "    #     X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_dem.pkl'))\n",
    "    \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "def train_3x3_dem_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_dem.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_dem.pkl')\n",
    "    else:\n",
    "        trial = int(len(imgfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                for i in range(N):\n",
    "                    for j in range(N):\n",
    "                        X_28.append(image[i][j])\n",
    "            dem_data = tifffile.imread(train_dem_files[box])\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    X_28.append(dem_data[i*3+1][j*3+1])\n",
    "            Y_28 = file_split[5].split(\".\")[0]\n",
    "            point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "            filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "            X.append(X_28[box*29*N*N: (box+1)*29*N*N])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "#             if box==100:break\n",
    "\n",
    "        del X_28, Y_28, point_28\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_dem.pkl'))\n",
    "    \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "        \n",
    "def test_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_testset.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_testset.pkl')\n",
    "    else:\n",
    "        trial = int(len(testfiles)/28)\n",
    "    #     trial = 100\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in testfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "\n",
    "            Y_28 = int(file_split[4].split(\".\")[0]) - 1\n",
    "    #         print(file_split, Y_28)\n",
    "            point_28 = [float(file_split[0]), float(file_split[1])]\n",
    "            filenames.append(f\"test_{file_split[0]}_{file_split[1]}_{labels[Y_28]}\")\n",
    "            X.append(X_28[box*28: (box+1)*28])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_testset.pkl'))\n",
    "        \n",
    "    return df\n",
    "def test_transform():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'):\n",
    "        df_test = pkl_loader(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl')\n",
    "        print(\"test_df をインポートしたよ！\")\n",
    "    else:\n",
    "        print(\"pklを新しく作成中ですが，標準化・正規化のコードに書き直しましたか？\")\n",
    "        X_test_zeros = np.zeros((X_test.shape[0], N*N*28))\n",
    "        for i in range(len(X_test)):\n",
    "            for k in range(N):\n",
    "                for l in range(N):\n",
    "                    for j in range(28):\n",
    "                        X_test_zeros[i][j+k+l] = X_test[i][k][l][j]\n",
    "        # X_test の名前が重複していたので違う名前に変えた. X_test_tmp\n",
    "        X_test_tmp = X_test_zeros\n",
    "        df_test = pd.concat([make_df(Y_test), make_df(X_test_tmp)], axis=1)\n",
    "        pkl_saver(df_test, os.path.join(data_path, f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'))\n",
    "        \n",
    "    return df_test.iloc[:, 1:], make_df(Y_test)\n",
    "def test_dem_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_test_dem.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_test_dem.pkl')\n",
    "    else:\n",
    "        trial = int(len(testfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for testfile in testfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(testfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(testfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image[0][0])\n",
    "            dem_data = tifffile.imread(test_dem_files[box])[0][0]\n",
    "            X_28.append(dem_data)\n",
    "            Y_28 = file_split[4].split(\".\")[0]\n",
    "            point_28 = [float(file_split[0]), float(file_split[1])]\n",
    "            filenames.append(f\"{box}_{file_split[0]}_{file_split[1]}_{Y_28}\")\n",
    "            X.append(X_28[box*29: (box+1)*29])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "    #         if box==100:break\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "    #     X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_test_dem.pkl'))\n",
    "        \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "def test_3x3_dem_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_test_dem.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_test_dem.pkl')\n",
    "    else:\n",
    "        trial = int(len(testfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for testfile in testfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(testfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(testfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                for i in range(N):\n",
    "                    for j in range(N):\n",
    "                        X_28.append(image[i][j])\n",
    "            dem_data = tifffile.imread(test_dem_files[box])\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    X_28.append(dem_data[i*3+1][j*3+1])\n",
    "            Y_28 = file_split[4].split(\".\")[0]\n",
    "            point_28 = [float(file_split[0]), float(file_split[1])]\n",
    "            filenames.append(f\"{box}_{file_split[0]}_{file_split[1]}_{Y_28}\")\n",
    "            X.append(X_28[box*29*N*N: (box+1)*29*N*N])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "#             if box==100:break\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_test_dem.pkl'))\n",
    "        \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "\n",
    "def data_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_test = Y[test_index]\n",
    "    X_train = X[train_index]\n",
    "    Y_train = Y[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "# Loss Definition ----------------------------------\n",
    "def opt_cnn(trial):\n",
    "    # Opt params -----------------------\n",
    "    # Categorical parameter\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 2)\n",
    "    dense_num = trial.suggest_int('dense_num', 3, 7)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(f'num_filter_{i}', 7, 10, 1)) for i in range(num_layer)]\n",
    "    size_filters = [int(trial.suggest_discrete_uniform(f'size_filter_{i}', 3, 3, 1)) for i in range(num_layer)]\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 5)\n",
    "    # Model Compiler -----------------------\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    decay = trial.suggest_loguniform('decay', 1e-6, 1e-3)\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = trial.suggest_discrete_uniform('dropout_rate_in', 0.0, 0.5, 0.1)\n",
    "    dropout_rate_out = trial.suggest_discrete_uniform('dropout_rate_out', 0.0, 0.5, 0.1)\n",
    "    momentum = trial.suggest_discrete_uniform('momentum', 0.0, 0.5, 0.1)\n",
    "    # categorical parameter\n",
    "#    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"rmsprop\", \"adam\"])\n",
    "    padding = \"same\"\n",
    "#     padding = trial.suggest_categorical('padding', ['same'])\n",
    "    # compile model-------------------\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True)\n",
    "#    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "    # For CPU run ------------------\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train Model ----------------------------------\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 0)\n",
    "    pr_cb = integration.TFKerasPruningCallback(trial, 'val_loss')\n",
    "    cbs = [es_cb, pr_cb]\n",
    "    loss_list, acc_list = [], []\n",
    "    for inner_cv in range(0, inner_cvs):\n",
    "        print(\"outer_cv =\", outer_cv, \"    inner_cv = \", inner_cv)\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = data_splitter_cv(train_files, X_outer_train, Y_outer_train, inner_cv, val_train_region, val_train_point)\n",
    "        hist = model.fit(\n",
    "            train_datagen.flow(X_inner_train, Y_inner_train, batch_size = (2**batch_size) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_inner_val, Y_inner_val),\n",
    "            callbacks = cbs,\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False)\n",
    "        loss_list += [model.evaluate(X_inner_val, Y_inner_val)[0]]\n",
    "        acc_list += [model.evaluate(X_inner_val, Y_inner_val)[1]]\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    eval_loss = np.mean(loss_list)\n",
    "    eval_acc = np.mean(acc_list)\n",
    "    print(\"eval_acc = \", eval_acc)\n",
    "    return eval_loss\n",
    "\n",
    "def create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out):\n",
    "    inputs = Input(image_shape)\n",
    "    for d in gpu_list:\n",
    "        with tf.device(d):\n",
    "            x = Dropout(dropout_rate_in)(inputs)\n",
    "            x = Convolution2D(filters = 2**num_filters[0], kernel_size = (size_filters[0],size_filters[0]), padding = \"same\", activation = 'relu')(x)\n",
    "            for i in range(1, num_layer):\n",
    "                x = Convolution2D(filters = 2**num_filters[i],\n",
    "                                  kernel_size = (size_filters[i], size_filters[i]),\n",
    "                                  padding = padding,\n",
    "                                  activation = 'relu')(x)\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dropout(dropout_rate_out)(x)\n",
    "            x = Dense(units = 2**dense_num, activation = 'relu')(x)\n",
    "            x = Dense(units = num_category, activation = 'softmax')(x)\n",
    "            model = Model(inputs = inputs, outputs = x)\n",
    "    return model\n",
    "\n",
    "def mean_params_calc(param_names):\n",
    "    dict = {}\n",
    "    categoricals = ['padding']\n",
    "    for param_name in param_names:\n",
    "        data_num = 0\n",
    "        if param_name not in categoricals:\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    try:\n",
    "                        dict[param_name] += data[param_name]\n",
    "                    except:\n",
    "                        dict[param_name] = data[param_name]\n",
    "                    data_num = data_num + 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = dict[param_name]/data_num\n",
    "        else:\n",
    "            categorical_list = []\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    categorical_list = categorical_list + [data[param_name]]\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = stats.mode(categorical_list)[0][0]\n",
    "    return dict\n",
    "\n",
    "def cv_result_imgs_generator(model, history):\n",
    "    # Visualize Loss Results ----------------------------\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\", marker=\"o\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\", marker=\"o\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(color='gray', alpha=0.2)\n",
    "    plt.savefig('./img_loss/' + str(outer_cv) + '_loss.jpg')\n",
    "    plt.close()\n",
    "\n",
    "def region_image_generator(point, region):\n",
    "    data_num = int(len(imgfiles)/28)\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(point[:data_num][:,0],point[:data_num][:,1], marker='o', s=5, color=cmap(region))\n",
    "    ax.set_title(\"Region in Japan\")\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")\n",
    "    ax.set_xlim([120, 155])\n",
    "    ax.set_ylim([20, 50])\n",
    "    fig.savefig('./region_separate.png')\n",
    "\n",
    "def make_dirs(model_name):\n",
    "    base_path = os.path.join(result_path , model_name)\n",
    "    # dir generation\n",
    "    dir_generator(base_path)\n",
    "    # Chenge current directry\n",
    "    os.mkdir(os.path.join(base_path, timename))\n",
    "    os.chdir(os.path.join(base_path, timename))\n",
    "    dir_generator(model_path)\n",
    "    dir_generator(\"./results/\")\n",
    "    dir_generator(\"./img_loss/\")\n",
    "    dir_generator(\"./model/\")\n",
    "    dir_generator(\"./weights/\")\n",
    "    dir_generator(\"./logs/\")\n",
    "    dir_generator(\"./outer_cv_times/\")\n",
    "\n",
    "def time_printer(start_time):\n",
    "    end_time = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {end_time - start_time} seconds.\"\n",
    "    \n",
    "# LightGBM ----------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "def opt_lgb(trial):\n",
    "    if lgb_boosting_type == \"gbdt\":\n",
    "        param_grid_lgb = {\n",
    "    #         \"device\": \"gpu\",\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 15),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    elif lgb_boosting_type ==\"rf\":\n",
    "        param_grid_lgb = {\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 15),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    \n",
    "    os.makedirs(f\"./results/outer_{outer_cv}\", exist_ok=True)\n",
    "    scores = []\n",
    "    for inner_cv in range(inner_cvs):\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = lgb_splitter_cv(train_files, X_outer_train, Y_outer_train, outer_cv, val_train_region, val_train_point)\n",
    "\n",
    "        model = LGBMClassifier(**param_grid_lgb)\n",
    "        model.fit(X_inner_train, Y_inner_train)\n",
    "        \n",
    "        scores.append(model.score(X_inner_val, Y_inner_val))\n",
    "        \n",
    "        # DEBUG -------------------------------------------------\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        inner_metr = make_df(confusion_matrix(Y_inner_val.astype(int).values, model.predict(X_inner_val).astype(int)))\n",
    "        inner_metr.to_csv(f\"./results/outer_{outer_cv}/inner_{inner_cv}_acc{round(model.score(X_inner_val, Y_inner_val), 2)}.csv\")\n",
    "    \n",
    "#     print('mean of inner_val_scores is ', np.mean(scores))\n",
    "    return np.mean(scores)\n",
    "def opt_lgb_random(trial):\n",
    "    if lgb_boosting_type == \"gbdt\":\n",
    "        param_grid_lgb = {\n",
    "    #         \"device\": \"gpu\",\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    elif lgb_boosting_type ==\"rf\":\n",
    "        param_grid_lgb = {\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    \n",
    "    os.makedirs(f\"./results/outer_{outer_cv}\", exist_ok=True)\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=inner_cvs, random_state=SEED, shuffle=False)\n",
    "    inner_cv = 0\n",
    "    X_valid  = X_outer_train\n",
    "    Y_valid  = Y_outer_train\n",
    "#     X_valid  = X_outer_train.values\n",
    "#     Y_valid  = Y_outer_train.values\n",
    "    for train_index, valid_index in skf.split(X_valid, Y_valid):\n",
    "        # DEBUG -------------------------------------------------\n",
    "#         from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        \n",
    "        X_inner_train, X_inner_val = X_outer_train[train_index], X_outer_train[valid_index]\n",
    "        Y_inner_train, Y_inner_val = Y_outer_train[train_index], Y_outer_train[valid_index]\n",
    "#         X_inner_train, X_inner_val = X_outer_train.to_numpy()[train_index], X_outer_train.to_numpy()[valid_index]\n",
    "#         Y_inner_train, Y_inner_val = Y_outer_train.to_numpy()[train_index], Y_outer_train.to_numpy()[valid_index]\n",
    "\n",
    "        model = LGBMClassifier(**param_grid_lgb)\n",
    "        model.fit(X_inner_train, Y_inner_train)\n",
    "        \n",
    "        scores.append(model.score(X_inner_val, Y_inner_val))\n",
    "        \n",
    "        inner_cv += 1\n",
    "#     print('mean of inner_val_scores is ', np.mean(scores))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def lgb_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = np.array(X)[test_index]\n",
    "    Y_test = np.array(Y)[test_index]\n",
    "    X_train = np.array(X)[train_index]\n",
    "    Y_train = np.array(Y)[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    X_train, X_test, Y_train, Y_test = make_df(X_train), make_df(X_test), make_df(Y_train), make_df(Y_test)\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "def make_df(X):\n",
    "    return pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df をインポートしたよ！\n",
      "test_df をインポートしたよ！\n"
     ]
    }
   ],
   "source": [
    "# Data Loader ------------------------------------------------------------------------\n",
    "N=3\n",
    "standarization = [\"normalization\", \"Zscore\", \"normal\"] \n",
    "# pklを作るまではCodeの書き換えも必要だよ！\n",
    "standarization_num= 0\n",
    "lgb_boosting_type = 'CNN'\n",
    "\n",
    "\n",
    "##### Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "testfiles.sort()\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "\n",
    "X_files, X_train, Y_train, train_point, region_train = train_import()\n",
    "X_train = X_train.astype(np.float64)\n",
    "image_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "num_category = len(np.unique(Y_train))\n",
    "\n",
    "Y_files, X_test, Y_test, test_point, region_test = test_import()\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "X_train, Y_train = train_transform()\n",
    "X_test , Y_test  = test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  <class 'pandas.core.frame.DataFrame'> (34088, 252)\n",
      "Y_train:  <class 'pandas.core.frame.DataFrame'> (34088, 1)\n",
      "X_test:  <class 'pandas.core.frame.DataFrame'> (3000, 252)\n",
      "Y_test:  <class 'pandas.core.frame.DataFrame'> (3000, 1)\n"
     ]
    }
   ],
   "source": [
    "# import data_shape check -------------------------------------------------------\n",
    "print(\"X_train: \", type(X_train), X_train.shape)\n",
    "print(\"Y_train: \", type(Y_train), Y_train.shape)\n",
    "print(\"X_test: \", type(X_test), X_test.shape)\n",
    "print(\"Y_test: \", type(Y_test), Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1808.6, 1814.8, 1807. ],\n",
       "        [1851.8, 1850.5, 1832.5],\n",
       "        [1887.8, 1886.8, 1865.5]],\n",
       "\n",
       "       [[1209.1, 1213.2, 1219.4],\n",
       "        [1221.8, 1223.3, 1226.6],\n",
       "        [1232.2, 1232.5, 1234.5]],\n",
       "\n",
       "       [[1262.9, 1237.8, 1228.3],\n",
       "        [1226.1, 1214.5, 1204.9],\n",
       "        [1216.3, 1211. , 1209.1]],\n",
       "\n",
       "       [[1368.5, 1371.8, 1376.9],\n",
       "        [1372.2, 1377.5, 1382.5],\n",
       "        [1353.2, 1356. , 1392.9]],\n",
       "\n",
       "       [[1209.1, 1213.2, 1219.4],\n",
       "        [1221.8, 1223.3, 1226.6],\n",
       "        [1232.2, 1232.5, 1234.5]],\n",
       "\n",
       "       [[1209.1, 1213.2, 1219.4],\n",
       "        [1221.8, 1223.3, 1226.6],\n",
       "        [1232.2, 1232.5, 1234.5]],\n",
       "\n",
       "       [[1209.1, 1213.2, 1219.4],\n",
       "        [1221.8, 1223.3, 1226.6],\n",
       "        [1232.2, 1232.5, 1234.5]],\n",
       "\n",
       "       [[1262.9, 1237.8, 1228.3],\n",
       "        [1226.1, 1214.5, 1204.9],\n",
       "        [1216.3, 1211. , 1209.1]],\n",
       "\n",
       "       [[1209.1, 1213.2, 1219.4],\n",
       "        [1221.8, 1223.3, 1226.6],\n",
       "        [1232.2, 1232.5, 1234.5]],\n",
       "\n",
       "       [[1262.9, 1237.8, 1228.3],\n",
       "        [1226.1, 1214.5, 1204.9],\n",
       "        [1216.3, 1211. , 1209.1]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prcp = glob(\"D:/LULC/features/10_JapanAverageDataset/3x3/prcp/*\")\n",
    "train_temp = glob(\"D:/LULC/features/10_JapanAverageDataset/3x3/temp/*\")\n",
    "test_prcp = glob(\"D:/LULC/features/10_JapanAverageDataset/3x3_test/prcp/*\")\n",
    "test_temp = glob(\"D:/LULC/features/10_JapanAverageDataset/3x3_test/temp/*\")\n",
    "\n",
    "def new_dataset(dataset):\n",
    "    lis = []\n",
    "    for data in dataset:\n",
    "        lis.append(tifffile.imread(data)[0][0])\n",
    "    return make_df(lis)\n",
    "\n",
    "# datasets = [train_prcp, train_temp]\n",
    "# X_new_train = pd.DataFrame()\n",
    "# for dataset in datasets:\n",
    "#     X_new_train = pd.concat([X_new_train, new_dataset(dataset)], axis=1)\n",
    "\n",
    "lis = []\n",
    "cnt=0\n",
    "for data in train_prcp:\n",
    "    lis.append(tifffile.imread(data))\n",
    "    cnt+=1\n",
    "    if cnt==10:\n",
    "        break\n",
    "np.array(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (1x1 pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Loader ------------------------------------------------------------------------\n",
    "# ================================== landsat8 Data Import ==================================\n",
    "N = 1\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "testfiles.sort()\n",
    "\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "\n",
    "train_path = os.path.join(data_path, f'df_1scene_lack_{N}x{N}_prcp_temp.pkl')\n",
    "test_path = os.path.join(data_path, f'df_1scene_lack_test_{N}x{N}_prcp_temp.pkl')\n",
    "if os.path.exists(train_path):\n",
    "    X_files, X_landsat_train, Y_train, train_point, region_train = pkl_loader(train_path)\n",
    "    Y_files, X_landsat_test, Y_test, test_point, region_test = pkl_loader(test_path)\n",
    "else:\n",
    "    # ------------train import----------------------\n",
    "    X_files, X_landsat_train, Y_train, train_point, region_train = train_import()\n",
    "    # image_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "    num_category = len(np.unique(Y_train))\n",
    "    X_landsat_train = X_landsat_train.astype(np.float64)\n",
    "    Y_train = Y_train.astype(np.float64)\n",
    "    # ------------test import----------------------\n",
    "    Y_files, X_landsat_test, Y_test, test_point, region_test = test_import()\n",
    "    X_landsat_test = X_landsat_test.astype(np.float64)\n",
    "\n",
    "    X_train = pd.DataFrame()\n",
    "    for i in tqdm(range(len(X_landsat_train))):\n",
    "        X_train = pd.concat([X_train, make_df(X_landsat_train[i][0][0])], axis=1)\n",
    "    X_train = X_train.T\n",
    "\n",
    "    X_test = pd.DataFrame()\n",
    "    for i in tqdm(range(len(X_landsat_test))):\n",
    "        X_test = pd.concat([X_test, make_df(X_landsat_test[i][0][0])], axis=1)\n",
    "    X_test = X_test.T\n",
    "    X_train.index = [x for x in range(len(X_train))]\n",
    "    X_test.index = [x for x in range(len(X_test))]\n",
    "    # ==============================================================================================\n",
    "    # ================================== New Data Import ===========================================\n",
    "    train_prcp = glob(\"D:/LULC/features/10_JapanAverageDataset/1x1/prcp/*\")\n",
    "    train_temp = glob(\"D:/LULC/features/10_JapanAverageDataset/1x1/temp/*\")\n",
    "    test_prcp = glob(\"D:/LULC/features/10_JapanAverageDataset/1x1_test/prcp/*\")\n",
    "    test_temp = glob(\"D:/LULC/features/10_JapanAverageDataset/1x1_test/temp/*\")\n",
    "\n",
    "    def new_dataset(dataset):\n",
    "        lis = []\n",
    "        for data in dataset:\n",
    "            lis.append(tifffile.imread(data)[0][0])\n",
    "        return make_df(lis)\n",
    "\n",
    "    datasets = [train_prcp, train_temp]\n",
    "    X_new_train = pd.DataFrame()\n",
    "    for dataset in datasets:\n",
    "        X_new_train = pd.concat([X_new_train, round(new_dataset(dataset), 1)], axis=1)\n",
    "\n",
    "    X_new_train[X_new_train.iloc[:,0]<0] = 0\n",
    "    X_new_train[X_new_train.iloc[:,1]<0] = 0\n",
    "    print(\"X_new_train shape is\", X_new_train.shape)\n",
    "\n",
    "\n",
    "    datasets = [test_prcp, test_temp]\n",
    "    X_new_test = pd.DataFrame()\n",
    "    for dataset in datasets:\n",
    "        X_new_test = pd.concat([X_new_test, round(new_dataset(dataset), 1)], axis=1)\n",
    "\n",
    "    X_new_test[X_new_test.iloc[:,0]<0] = 0\n",
    "    X_new_test[X_new_test.iloc[:,1]<0] = 0\n",
    "    print(\"X_new_test shape is\", X_new_train.shape)\n",
    "\n",
    "    X_train = pd.concat([X_train, X_new_train], axis=1)\n",
    "    X_test = pd.concat([X_test, X_new_test], axis=1)\n",
    "\n",
    "    train_lack_df = [X_files, X_train, Y_train, train_point, region_train]\n",
    "    pkl_saver(train_lack_df, train_path)\n",
    "    test_lack_df = [Y_files, X_test, Y_test, test_point, region_test]\n",
    "    pkl_saver(test_lack_df, test_path)\n",
    "\n",
    "\n",
    "    # ----------------------------- REMOVE datalack of train and test-------------------------------------\n",
    "    lack_num = 9\n",
    "    X_files = make_df(X_files)\n",
    "    Y_train = make_df(Y_train)\n",
    "\n",
    "    del_index = []\n",
    "    for i in range(len(X_train)):\n",
    "        if sum(X_train.iloc[i,:]==0)>lack_num:\n",
    "            del_index.append(i)\n",
    "    X_files.drop(index=del_index, inplace=True)\n",
    "    X_train.drop(index=X_train.index[del_index], inplace=True)\n",
    "    Y_train.drop(index=del_index, inplace=True)\n",
    "\n",
    "    Y_files = make_df(Y_files)\n",
    "    X_test =  make_df(X_test)\n",
    "    Y_test = make_df(Y_test)\n",
    "    del_index = []\n",
    "    for i in range(len(X_test)):\n",
    "        if sum(X_test.iloc[i,:]==0)>lack_num:\n",
    "            del_index.append(i)\n",
    "    Y_files.drop(index=del_index, inplace=True)\n",
    "    X_test.drop(index=X_test.index[del_index], inplace=True)\n",
    "    Y_test.drop(index=del_index, inplace=True)\n",
    "\n",
    "\n",
    "# import data_shape check -------------------------------------------------------------\n",
    "print(\"X_train: \", type(X_train), X_train.shape)\n",
    "print(\"Y_train: \", type(Y_train), Y_train.shape)\n",
    "print(\"X_test: \", type(X_test), X_test.shape)\n",
    "print(\"Y_test: \", type(Y_test), Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs\n",
    "X_files = X_files.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_type =  \"random\"  # \"random\" or sparse\"\n",
    "make_dirs(lgb_boosting_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     68
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train start -----------------------------------------------------------------------------\n",
    "if CV_type == \"random\":\n",
    "    print(\"CV is RANDOM!!!\")\n",
    "    train_start = datetime.datetime.now()\n",
    "    # region_image_generator(train_point, region_train)\n",
    "    SKF = StratifiedKFold(n_splits=outer_cvs, random_state=SEED, shuffle=False)\n",
    "    outer_cv = 0\n",
    "    for train_index, valid_index in SKF.split(X_train, Y_train):\n",
    "        os.makedirs(f\"./results/outer_{outer_cv}\", exist_ok=True)\n",
    "        X_outer_train, X_outer_val = X_train.to_numpy()[train_index], X_train.to_numpy()[valid_index]\n",
    "        Y_outer_train, Y_outer_val = Y_train.to_numpy()[train_index], Y_train.to_numpy()[valid_index]\n",
    "\n",
    "        outer_start = datetime.datetime.now()\n",
    "        print(f'outer_cv_{outer_cv}_processing....')\n",
    "        # Data Loader-------------------------------------\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(opt_lgb_random, n_trials=n_trials)\n",
    "    #     print(study.best_value)\n",
    "\n",
    "        lgb_best_param = study.best_params\n",
    "        lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "        lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "\n",
    "        print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val[:,0].astype(int)).sum() / len(Y_outer_val) ) )\n",
    "        print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test.values).astype(int)      == Y_test.astype(int)[0].values).sum() / len(Y_test) ) )\n",
    "        #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "\n",
    "        try:\n",
    "            Y_val_files.append(X_files[valid_index])\n",
    "        except:\n",
    "            Y_val_files =  [X_files[valid_index]]    \n",
    "        try:\n",
    "            lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0]).sum() / len(Y_outer_val))\n",
    "        except:\n",
    "            lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0]).sum() / len(Y_outer_val)]\n",
    "        try:\n",
    "            lgb_best_params.append(lgb_best_param)\n",
    "        except:\n",
    "            lgb_best_params = [lgb_best_param]\n",
    "        try:\n",
    "            Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "        except:\n",
    "            Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "        try:\n",
    "            Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "        except:\n",
    "            Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "        try:\n",
    "            Y_val_obs.append(Y_outer_val.astype(int))\n",
    "        except:\n",
    "            Y_val_obs =  [Y_outer_valastype(int)]\n",
    "\n",
    "        outer_end = datetime.datetime.now()\n",
    "        spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "        pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "\n",
    "        outer_metr = make_df(confusion_matrix(Y_outer_val.astype(int), lgb_best.predict(X_outer_val).astype(int)))\n",
    "        outer_metr.to_csv(f\"./results/outer_{outer_cv}/outer_{outer_cv}_acc{round(lgb_best.score(X_outer_val, Y_outer_val), 2)}.csv\")\n",
    "        test_metr = make_df(confusion_matrix(Y_test.astype(int), lgb_best.predict(X_test).astype(int)))\n",
    "        test_metr.to_csv(f\"./results/outer_{outer_cv}/test_acc{round(lgb_best.score(X_test, Y_test), 2)}.csv\")\n",
    "        outer_cv += 1\n",
    "\n",
    "    train_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))\n",
    "\n",
    "elif CV_type == \"sparse\":\n",
    "    # train start -----------------------------------------------------------------------------\n",
    "    train_start = datetime.datetime.now()\n",
    "    region_image_generator(train_point, region_train)\n",
    "\n",
    "    for outer_cv in range(outer_cvs):\n",
    "        print(\"CV is SPARTIAL!!!\")\n",
    "        outer_start = datetime.datetime.now()\n",
    "        print(f'outer_cv_{outer_cv}_processing....')\n",
    "        # Data Loader-------------------------------------\n",
    "        train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "        val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_    \n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(opt_lgb, n_trials=n_trials)\n",
    "    #     print(study.best_value)\n",
    "\n",
    "        lgb_best_param = study.best_params\n",
    "        lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "        lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "\n",
    "        print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val) ) )\n",
    "        print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test).astype(int)      == Y_test.astype(int)[0].values).sum() / len(Y_test) ) )\n",
    "        #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "\n",
    "        try:\n",
    "            Y_val_files.append(val_files)\n",
    "        except:\n",
    "            Y_val_files =  [val_files]    \n",
    "        try:\n",
    "            lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val))\n",
    "        except:\n",
    "            lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val)]\n",
    "        try:\n",
    "            lgb_best_params.append(lgb_best_param)\n",
    "        except:\n",
    "            lgb_best_params = [lgb_best_param]\n",
    "        try:\n",
    "            Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "        except:\n",
    "            Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "        try:\n",
    "            Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "        except:\n",
    "            Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "        try:\n",
    "            Y_val_obs.append(Y_outer_val[0].values.astype(int))\n",
    "        except:\n",
    "            Y_val_obs =  [Y_outer_val[0].values.astype(int)]\n",
    "\n",
    "        outer_end = datetime.datetime.now()\n",
    "        spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "        pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "    train_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))\n",
    "    \n",
    "else: print(\"Please enter the correct CV name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y_val_obs\n",
    "for train_index, valid_index in SKF.split(X_train, Y_train):\n",
    "    Y_outer_train, Y_outer_val = Y_train.to_numpy()[train_index], Y_train.to_numpy()[valid_index]\n",
    "    try:\n",
    "        Y_val_obs.append(Y_outer_val.astype(int))\n",
    "    except:\n",
    "        Y_val_obs =  [Y_outer_val.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_outer_val.astype(int).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_df(Y_val_files[best_trial_num]).shape)\n",
    "print(make_df(Y_val_obs[best_trial_num]).shape)\n",
    "print(make_df(Y_val_pred[best_trial_num]).shape)\n",
    "print(make_df(Y_val_smx[:][best_trial_num]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(lgb_best.predict(X_train).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save 6_classes Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "# results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "# pkl_saver(results, './results/results.pkl')\n",
    "# make_df(val_files)\n",
    "# results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "# results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Crops', 'Grassland', \"Forest\", 'Bare land']\n",
    "# results_csv.columns=columns\n",
    "# results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Crops', 'Grassland', \"Forest\", 'Bare land']\n",
    "\n",
    "# res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "# with open('./results/result_summary_val.txt','w') as f:\n",
    "#     f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "# results_csv.columns=columns\n",
    "# results_csv.to_csv('./results/results_test.csv')\n",
    "cf_metr = confusion_matrix(Y_test.astype(int).values, best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# ugokanaikamo \n",
    "LGB.plot_importance(best_model, height = 0.5, figsize = (8,16))\n",
    "plt.savefig('./figure.png')\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "pickle.dump(best_model, open(\"./best_model.pkl\", 'wb'))\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Save Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "# make_df(val_files)\n",
    "results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int).values, best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# ugokanaikamo \n",
    "LGB.plot_importance(best_model, height = 0.5, figsize = (8,16))\n",
    "plt.savefig('./figure.png')\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "pickle.dump(best_model, open(\"./best_model.pkl\", 'wb'))\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_model = pickle.load(open('./best_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Show Accuracy --------------------------------------------------------------------------\n",
    "acc = round(np.array(read_model.predict(X_test).astype(int) == Y_test.astype(int)[0].values).sum() / len(Y_test), 3)*100\n",
    "print(acc)\n",
    "if CV_type == \"random\":\n",
    "    pkl_saver(acc, f'randomCV_acc_{acc}.txt')\n",
    "elif CV_type == \"sparse\":\n",
    "    pkl_saver(acc, f'spartialCV_acc_{acc}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time_name = \"2020_1209_1909\"\n",
    "os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}\")\n",
    "read_model = pickle.load(open('./best_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# SAVING OBS but PRED VISUALIZATIONS ------------------------------------\n",
    "pred = read_model.predict(X_test).astype(int)\n",
    "obs = Y_test.astype(int)[0].values\n",
    "lon, lat = [], []\n",
    "for i in range(len(Y_files)):\n",
    "    lon.append(float(Y_files.values[i][0].split(\"_\")[1]))\n",
    "    lat.append(float(Y_files.values[i][0].split(\"_\")[2]))\n",
    "os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization\", exist_ok=True)\n",
    "os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization\")\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "\n",
    "df_lon = make_df(lon)\n",
    "df_lat = make_df(lat)\n",
    "df_pred = make_df(pred)\n",
    "df_obs = make_df(obs)\n",
    "results = pd.concat([df_lon, df_lat, df_pred, df_obs], axis=1)\n",
    "results.columns = [\"lon\", \"lat\", \"pred\", \"obs\"]\n",
    "\n",
    "os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/csv\", exist_ok=True)\n",
    "os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/csv\")\n",
    "results[results[\"pred\"]!=results[\"obs\"]].to_csv(\"./miss_all.csv\")\n",
    "for i in range(10):\n",
    "    obs_lis = results[results[\"pred\"]!=results[\"obs\"]][results[\"obs\"]==i]\n",
    "    pred_lis = results[results[\"pred\"]!=results[\"obs\"]][results[\"pred\"]==i]\n",
    "    obs_lis.to_csv(f\"./obs_{i}.csv\")\n",
    "    pred_lis.to_csv(f\"./pred_{i}.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_point_check\", exist_ok=True)\n",
    "os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_point_check\")\n",
    "for i in range(10):\n",
    "    obs_lis = results[results[\"pred\"]!=results[\"obs\"]][results[\"obs\"]==i]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    for j in range(10):\n",
    "        pred_j = results[results[\"pred\"]!=results[\"obs\"]][results[\"obs\"]==i][results[\"pred\"]==j]\n",
    "        if len(pred_j)==0: continue\n",
    "        ax.set_title(f'OBS = {labels[i]}')\n",
    "        ax.scatter(pred_j[\"lon\"].values.astype(np.float64), pred_j[\"lat\"].values.astype(np.float64), label=labels[j], s=10)\n",
    "        ax.set_xlim([120, 155])\n",
    "        ax.set_ylim([20, 50])\n",
    "        \n",
    "        ax.legend(loc='upper left')\n",
    "        plt.savefig(f\"./obs_{labels[i]}.png\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "pred = read_model.predict(X_test).astype(int)\n",
    "obs = Y_test.astype(int)[0].values\n",
    "lon, lat = [], []\n",
    "for i in range(len(Y_files)):\n",
    "    lon.append(float(Y_files.values[i][0].split(\"_\")[1]))\n",
    "    lat.append(float(Y_files.values[i][0].split(\"_\")[2]))\n",
    "\n",
    "df_lon = make_df(lon)\n",
    "df_lat = make_df(lat)\n",
    "df_pred = make_df(pred)\n",
    "df_obs = make_df(obs)\n",
    "results = pd.concat([df_lon, df_lat, df_pred, df_obs], axis=1)\n",
    "results.columns = [\"lon\", \"lat\", \"pred\", \"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# MISS BAND CHECK --------------------------------\n",
    "band = 4\n",
    "for band in range(7):\n",
    "    DE_check = make_df(X_test).iloc[:,band::7]\n",
    "    make_df(DE_check)\n",
    "    os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check\", exist_ok=True)\n",
    "    os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check\")\n",
    "\n",
    "    # 1-index\n",
    "    seasons = [\"Summer\", \"Spring\", \"Autumn\", \"Winter\"]\n",
    "    seasons.sort()\n",
    "    for i in range(10):\n",
    "        os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check/obs_{labels[i]}\", exist_ok=True)\n",
    "        obs_true_lis = make_df(DE_check.to_numpy()[(results[\"pred\"]==results[\"obs\"]) * (results[\"obs\"]==i)])\n",
    "        obs_miss_lis = make_df(DE_check.to_numpy()[(results[\"pred\"]!=results[\"obs\"]) * (results[\"obs\"]==i)])\n",
    "    #     for j in range(4):\n",
    "    #         fig = plt.figure()\n",
    "    #         ax = fig.add_subplot(1,1,1)\n",
    "    #         ax.set_title(f\"OBS = {labels[i]}_{seasons[j]}_BAND5\")\n",
    "    #         ax.hist(obs_true_lis.iloc[:, j].values, label=\"TRUE\", color=\"g\", alpha=0.3, range=(-2000, 12000), bins=28)\n",
    "\n",
    "    #         ax.hist(obs_miss_lis.iloc[:, j].values, label=\"MISS\", color=\"r\", alpha=0.3, range=(-2000, 12000), bins=28)\n",
    "    #         ax.set_xlim([-2000, 12000])\n",
    "    #         ax.set_ylim([0, 200])\n",
    "    #         ax.axvline(0, ls = \"--\", color = \"navy\")\n",
    "    #         ax.legend(loc='upper left')\n",
    "    #         plt.savefig(f\"./obs_{labels[i]}/{seasons[j]}_band_5.png\")\n",
    "\n",
    "        upper = 0\n",
    "        lower = 3000\n",
    "        \n",
    "        if i==6:\n",
    "            look = 7\n",
    "            aaa = make_df(DE_check.to_numpy()[(results[\"obs\"]==look)])\n",
    "            bbb = make_df(DE_check.to_numpy()[(results[\"obs\"]==i) * (results[\"pred\"]==look)])\n",
    "            for j in range(4):\n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(1,1,1)\n",
    "                fig.patch.set_facecolor('white')  # 図全体の背景色\n",
    "                fig.patch.set_alpha(0)  # 図全体の背景透明度\n",
    "                ax.patch.set_facecolor('white')  # subplotの背景色\n",
    "                ax.patch.set_alpha(0)  # subplotの背景透明度\n",
    "                #一緒obs=pred\n",
    "#                 ax.set_title(f\"OBS = {labels[i]}_{seasons[j]}_BAND{band+1}\")\n",
    "                ax.hist(obs_true_lis.iloc[:, j].values, label=f\"OBS_{labels[i]}_PRED_{labels[i]}\", color=\"g\", alpha=0.5, range=(lower, upper), bins=30)\n",
    "\n",
    "                #一緒obs=pred\n",
    "                ax.hist(aaa.iloc[:, j].values, label=f\"OBS_{labels[look]}_PRED_{labels[look]}\", color=\"r\", alpha=0.3, range=(lower, upper), bins=30)\n",
    "                ax.set_xlim([lower, upper])\n",
    "                ax.set_ylim([0, 200])\n",
    "#                 ax.axvline(0, ls = \"--\", color = \"navy\")\n",
    "#                 ax.legend(loc='upper left')\n",
    "\n",
    "                #違うobs!=pred\n",
    "                ax.hist(bbb.iloc[:, j].values, label=f\"OBS_{labels[i]}_PRED_{labels[look]}\", color=\"b\", alpha=0.4, range=(lower, upper), bins=30)\n",
    "                ax.set_xlim([lower, upper])\n",
    "                ax.set_ylim([0, 200])\n",
    "#                 ax.axvline(0, ls = \"--\", color = \"navy\")\n",
    "#                 ax.legend(loc='upper left')\n",
    "\n",
    "                os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check/{labels[i]}_and_{labels[look]}\", exist_ok=True)\n",
    "                plt.savefig(f\"./{labels[i]}_and_{labels[look]}/{seasons[j]}_band{band+1}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MISS BAND CHECK --------------------------------\n",
    "for band in range(7):\n",
    "    DE_check = make_df(X_test).iloc[:,band::7]\n",
    "    make_df(DE_check)\n",
    "    os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check\", exist_ok=True)\n",
    "    os.chdir(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check\")\n",
    "\n",
    "    # 1-index\n",
    "    seasons = [\"Summer\", \"Spring\", \"Autumn\", \"Winter\"]\n",
    "    seasons.sort()\n",
    "    \n",
    "    os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check/obs_{labels[i]}\", exist_ok=True)\n",
    "    obs_true_dbf = make_df(DE_check.to_numpy()[(results[\"pred\"]==results[\"obs\"]) * (results[\"obs\"]==5)])\n",
    "    obs_miss_dbf = make_df(DE_check.to_numpy()[(results[\"pred\"]!=results[\"obs\"]) * (results[\"obs\"]==5)])\n",
    "\n",
    "    look = 1\n",
    "    aaa = make_df(DE_check.to_numpy()[(results[\"obs\"]==look)])\n",
    "    bbb = make_df(DE_check.to_numpy()[(results[\"obs\"]==i) * (results[\"pred\"]==look)])\n",
    "    for j in range(4):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        #一緒obs=pred\n",
    "        ax.set_title(f\"OBS = {labels[i]}_{seasons[j]}_BAND{band+1}\")\n",
    "        ax.hist([obs_true_lis.iloc[:, j], .values, label=f\"OBS_{labels[i]}_PRED_{labels[i]}\", color=\"g\", alpha=0.5, range=(-2000, 12000), bins=28)\n",
    "\n",
    "        #一緒obs=pred\n",
    "        ax.hist(aaa.iloc[:, j].values, label=f\"OBS_{labels[look]}_PRED_{labels[look]}\", color=\"r\", alpha=0.3, range=(-2000, 12000), bins=28)\n",
    "        ax.set_xlim([-2000, 12000])\n",
    "        ax.set_ylim([0, 200])\n",
    "        ax.axvline(0, ls = \"--\", color = \"navy\")\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        #違うobs!=pred\n",
    "        ax.hist(bbb.iloc[:, j].values, label=f\"OBS_{labels[i]}_PRED_{labels[look]}\", color=\"b\", alpha=0.4, range=(-2000, 12000), bins=28)\n",
    "        ax.set_xlim([-2000, 12000])\n",
    "        ax.set_ylim([0, 200])\n",
    "        ax.axvline(0, ls = \"--\", color = \"navy\")\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "#                 os.makedirs(f\"C:/Users/GE/Dropbox/Kairo/under90_results/1x1/gbdt/{file_time_name}/visualization/miss_band_check/{labels[i]}_and_{labels[look]}\", exist_ok=True)\n",
    "#                 plt.savefig(f\"./{labels[i]}_and_{labels[look]}/{seasons[j]}_band{band+1}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = round(np.array(read_model.predict(X_train).astype(int) == Y_train.astype(int)[0].values).sum() / len(Y_train), 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM + DEM (1x1 pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data Loader ------------------------------------------------------------------------\n",
    "N=1\n",
    "standarization = [\"normalization\", \"Zscore\", \"normal\", \"dem\"] # pklを作るまではCodeの書き換えも必要だよ！\n",
    "standarization_num= 3\n",
    "\n",
    "##### Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "testfiles.sort()\n",
    "\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "train_dem_files = glob(os.path.join(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_dem\", \"*\"))\n",
    "test_dem_files = glob(os.path.join(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test_dem\", \"*\"))\n",
    "train_dem_files.sort()\n",
    "test_dem_files.sort()\n",
    "model_trained = False\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}_dem'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "\n",
    "X_files, X_train, Y_train, train_point, region_train = train_dem_import()\n",
    "X_train = X_train.astype(np.float64)\n",
    "Y_train = Y_train.astype(np.float64)\n",
    "\n",
    "Y_files, X_test, Y_test, test_point, region_test = test_dem_import()\n",
    "X_test = X_test.astype(np.float64)\n",
    "Y_test = Y_test.astype(np.float64) - 1\n",
    "\n",
    "\n",
    "# # Data converter ----------------------------------------------\n",
    "X_train = make_df(X_train)\n",
    "Y_train = make_df(Y_train)\n",
    "X_test =  make_df(X_test)\n",
    "Y_test = make_df(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# REMOVE datalack of train and test\n",
    "del_index = []\n",
    "for i in range(len(X_train)):\n",
    "    if sum(X_train.loc[i]==0)>7:\n",
    "        del_index.append(i)\n",
    "train_files.drop(index=del_index, inplace=True)\n",
    "X_train.drop(index=del_index, inplace=True)\n",
    "Y_train.drop(index=del_index, inplace=True)\n",
    "\n",
    "del_index = []\n",
    "for i in range(len(X_test)):\n",
    "    if sum(X_test.loc[i]==0)>7:\n",
    "        del_index.append(i)\n",
    "test_files.drop(index=del_index, inplace=True)\n",
    "X_test.drop(index=del_index, inplace=True)\n",
    "Y_test.drop(index=del_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import data_shape check --------------------------------------------------------\n",
    "print(\"X_train: \", type(X_train), X_train.shape)\n",
    "print(\"Y_train: \", type(Y_train), Y_train.shape)\n",
    "print(\"X_test: \", type(X_test), X_test.shape)\n",
    "print(\"Y_test: \", type(Y_test), Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train start -----------------------------------------------------------------------------\n",
    "train_start = datetime.datetime.now()\n",
    "\n",
    "for outer_cv in range(outer_cvs):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_    \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(opt_lgb, n_trials=n_trials)\n",
    "#     print(study.best_value)\n",
    "    \n",
    "    lgb_best_param = study.best_params\n",
    "    lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "    lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "    \n",
    "    print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val) ) )\n",
    "    print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test).astype(int)      == Y_test.astype(int)[0].values).sum() / len(Y_test) ) )\n",
    "    #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "    \n",
    "    try:\n",
    "        Y_val_files.append(val_files)\n",
    "    except:\n",
    "        Y_val_files =  [val_files]    \n",
    "    try:\n",
    "        lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val))\n",
    "    except:\n",
    "        lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val)]\n",
    "    try:\n",
    "        lgb_best_params.append(lgb_best_param)\n",
    "    except:\n",
    "        lgb_best_params = [lgb_best_param]\n",
    "    try:\n",
    "        Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "    except:\n",
    "        Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "    try:\n",
    "        Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "    except:\n",
    "        Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "    try:\n",
    "        Y_val_obs.append(Y_outer_val[0].values.astype(int))\n",
    "    except:\n",
    "        Y_val_obs =  [Y_outer_val[0].values.astype(int)]\n",
    "\n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "train_end = datetime.datetime.now()\n",
    "spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(lgb_best.predict(X_test).astype(int)), np.unique(Y_test.astype(int)[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "make_df(val_files)\n",
    "results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "# cf_metr = confusion_matrix(Y_val_obs[best_trial_num].astype(int), Y_val_pred[best_trial_num])\n",
    "# cf_metr = pd.DataFrame(cf_metr)\n",
    "# cf_metr.columns=labels\n",
    "# cf_metr.index=labels\n",
    "# cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int).values, best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# ugokanaikamo \n",
    "LGB.plot_importance(best_model, height = 0.5, figsize = (8,16))\n",
    "plt.savefig('./figure.png')\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "# best_model.save('./model/best_model.hdf5')\n",
    "# model_trained = True\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Show Accuracy --------------------------------------------------------------------------\n",
    "acc = round(np.array(best_model.predict(X_test).astype(int) == Y_test.astype(int)[0].values).sum() / len(Y_test), 3)*100\n",
    "print(acc)\n",
    "pkl_saver(acc, f'acc_{acc}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (3x3 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data Loader ------------------------------------------------------------------------\n",
    "N=3\n",
    "standarization = [\"normalization\", \"Zscore\", \"normal\"] # pklを作るまではCodeの書き換えも必要だよ！\n",
    "standarization_num= 2\n",
    "\n",
    "##### Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "testfiles.sort()\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "\n",
    "X_files, X_train, Y_train, train_point, region_train = train_import()\n",
    "X_train = X_train.astype(np.float64)\n",
    "\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl'):\n",
    "    df_train = pkl_loader(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl')\n",
    "else:\n",
    "    X_train_zeros = np.zeros((X_train.shape[0], N*N*28))\n",
    "    for i in range(len(X_train)):\n",
    "        for k in range(3):\n",
    "            for l in range(3):\n",
    "                for j in range(28):\n",
    "                    X_train_zeros[i][j+k+l] = X_train[i][k][l][j]\n",
    "    X_train = X_train_zeros\n",
    "    df_train = pd.concat([make_df(Y_train), make_df(X_train)], axis=1)\n",
    "    pkl_saver(df_train, os.path.join(data_path, f'df_{N}x{N}_{standarization[standarization_num]}.pkl'))\n",
    "    \n",
    "\n",
    "Y_files, X_test, Y_test, test_point, region_test = test_import()\n",
    "X_test = X_test.astype(np.float64)\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'):\n",
    "    df_test = pkl_loader(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl')\n",
    "else:\n",
    "    X_test_zeros = np.zeros((X_test.shape[0], N*N*28))\n",
    "    for i in range(len(X_test)):\n",
    "        for k in range(3):\n",
    "            for l in range(3):\n",
    "                for j in range(28):\n",
    "                    X_test_zeros[i][j+k+l] = X_test[i][k][l][j]\n",
    "    X_test = X_test_zeros\n",
    "    df_test = pd.concat([make_df(Y_test), make_df(X_test)], axis=1)\n",
    "    pkl_saver(df_test, os.path.join(data_path, f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'))\n",
    "\n",
    "# Data converter ----------------------------------------------\n",
    "X_train = df_train.iloc[:, 1:]\n",
    "Y_train = make_df(Y_train)\n",
    "X_test =  df_test.iloc[:, 1:]\n",
    "Y_test = make_df(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import data_shape check --------------------------------------------------------\n",
    "print(\"X_train: \", type(X_train), X_train.shape)\n",
    "print(\"Y_train: \", type(Y_train), Y_train.shape)\n",
    "print(\"X_test: \", type(X_test), X_test.shape)\n",
    "print(\"Y_test: \", type(Y_test), Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train start -----------------------------------------------------------------------------\n",
    "train_start = datetime.datetime.now()\n",
    "\n",
    "for outer_cv in range(outer_cvs):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_    \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(opt_lgb, n_trials=n_trials)\n",
    "#     print(study.best_value)\n",
    "    \n",
    "    lgb_best_param = study.best_params\n",
    "    lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "    lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "    \n",
    "    print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val) ) )\n",
    "    print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test).astype(int)      == Y_test.astype(int)[0].values).sum() / len(Y_test) ) )\n",
    "    #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "    \n",
    "    try:\n",
    "        Y_val_files.append(val_files)\n",
    "    except:\n",
    "        Y_val_files =  [val_files]    \n",
    "    try:\n",
    "        lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val))\n",
    "    except:\n",
    "        lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val)]\n",
    "    try:\n",
    "        lgb_best_params.append(lgb_best_param)\n",
    "    except:\n",
    "        lgb_best_params = [lgb_best_param]\n",
    "    try:\n",
    "        Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "    except:\n",
    "        Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "    try:\n",
    "        Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "    except:\n",
    "        Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "    try:\n",
    "        Y_val_obs.append(Y_outer_val[0].values.astype(int))\n",
    "    except:\n",
    "        Y_val_obs =  [Y_outer_val[0].values.astype(int)]\n",
    "\n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "train_end = datetime.datetime.now()\n",
    "spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "make_df(val_files)\n",
    "results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "# cf_metr = confusion_matrix(Y_val_obs[best_trial_num].astype(int), Y_val_pred[best_trial_num])\n",
    "# cf_metr = pd.DataFrame(cf_metr)\n",
    "# cf_metr.columns=labels\n",
    "# cf_metr.index=labels\n",
    "# cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int).values, best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# ugokanaikamo \n",
    "LGB.plot_importance(best_model, height = 0.5, figsize = (8,16))\n",
    "plt.savefig('./figure.png')\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "# best_model.save('./model/best_model.hdf5')\n",
    "# model_trained = True\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Show Accuracy --------------------------------------------------------------------------\n",
    "acc = round(np.array(best_model.predict(X_test).astype(int) == Y_test.astype(int)[0].values).sum() / len(Y_test), 3)*100\n",
    "print(acc)\n",
    "pkl_saver(acc, f'acc_{acc}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.955px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
