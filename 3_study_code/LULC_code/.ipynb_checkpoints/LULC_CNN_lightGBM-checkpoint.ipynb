{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "under = 90\n",
    "model = \"lgb\"   #lgb or cnn\n",
    "gpu_list = ['/gpu:0', '/gpu:1']\n",
    "# Training params ------------------------------------------\n",
    "train_epochs = 2**2\n",
    "ntrials = 2**4\n",
    "cvs = 10\n",
    "best_epochs = 2**5\n",
    "early_stopping = 2**3\n",
    "\n",
    "\n",
    "# Utils -----------------------\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, zipfile, io, re\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "# Machine Learning ---------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from math import sqrt\n",
    "import optuna\n",
    "from optuna import integration\n",
    "import optuna.integration.lightgbm as lgb\n",
    "# Keras, TensorFlow ---------------\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization, Convolution2D, Input\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tifffile\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 31\n",
    "np.random.seed(SEED)\n",
    "gpus = len(gpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# plt.tick_params(colors='white')\n",
    "# IO Functions ------------------------------\n",
    "def pkl_saver(object, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as web:\n",
    "        pickle.dump(object , web)\n",
    "\n",
    "\n",
    "def pkl_loader(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as web:\n",
    "        data = pickle.load(web)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Dir generator ----------------------------\n",
    "def dir_generator(dir_path):\n",
    "    if os.path.exists(dir_path) == False:\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "def region_visualizer(df):\n",
    "    points = df[3]\n",
    "    lon, lat = [], []\n",
    "    for point in points:\n",
    "        lon.append(point[0])\n",
    "        lat.append(point[1])\n",
    "    lon = np.array(lon).astype(float).reshape(-1,1)\n",
    "    lat = np.array(lat).astype(float).reshape(-1,1)\n",
    "    region = df[4]\n",
    "    region = np.array(region).astype(int).reshape(-1,1)\n",
    "    df = pd.DataFrame(np.concatenate([lon, lat, region], axis=1))\n",
    "    df.columns = ['longitude', 'latitude', 'region_class']\n",
    "    pivotted = df.pivot('longitude', 'latitude', 'region_class')\n",
    "    for i in range(pivotted.shape[0]):\n",
    "        pivotted.iloc[i] = pd.to_numeric(pivotted.iloc[i])\n",
    "    pivotted.columns = pd.to_numeric(pivotted.columns)\n",
    "    pivotted.index = pd.to_numeric(pivotted.index)\n",
    "    pivotted = pivotted.fillna(-1)\n",
    "    pivotted = pivotted.astype(float).T\n",
    "    cmap = sns.color_palette(\"deep\", cvs + 1)\n",
    "    cmap[0] = (0,0,0)\n",
    "    plt = sns.heatmap(pivotted, cmap = cmap)\n",
    "    plt.invert_yaxis()\n",
    "    colorbar = plt.collections[0].colorbar\n",
    "    r = colorbar.vmax - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + 0.5 * r / (cvs + 1) + r * i / (cvs + 1) for i in range(cvs + 1)])\n",
    "    colorbar.set_ticklabels(['background']+list(range(cvs)))\n",
    "    plt.figure.savefig(os.path.join(result_path, \"result_img\", \"region_map.jpg\"))\n",
    "    del(plt)\n",
    "\n",
    "\n",
    "def data_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_test = Y[test_index]\n",
    "    X_train = X[train_index]\n",
    "    Y_train = Y[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "def lgb_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = np.array(X)[test_index]\n",
    "    Y_test = np.array(Y)[test_index]\n",
    "    X_train = np.array(X)[train_index]\n",
    "    Y_train = np.array(Y)[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    X_train, X_test, Y_train, Y_test = make_df(X_train), make_df(X_test), make_df(Y_train), make_df(Y_test)\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "\n",
    "# Loss Definition ----------------------------------\n",
    "def root_mean_squared_error(Y_true, Y_pred):\n",
    "    return K.sqrt(K.mean(K.square(Y_pred - Y_true), axis = -1))\n",
    "\n",
    "\n",
    "def create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out):\n",
    "    inputs = Input(image_shape)\n",
    "    for d in gpu_list:\n",
    "        with tf.device(d):\n",
    "            x = Dropout(dropout_rate_in)(inputs)\n",
    "            x = Convolution2D(filters = 2**num_filters[0], kernel_size = (size_filters[0],size_filters[0]), padding = 'same', activation = 'relu')(x)\n",
    "            for i in range(1, num_layer):\n",
    "                x = Convolution2D(filters = 2**num_filters[i],\n",
    "                                  kernel_size = (size_filters[i], size_filters[i]),\n",
    "                                  padding = padding,\n",
    "                                  activation = 'relu')(x)\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dropout(dropout_rate_out)(x)\n",
    "            x = Dense(units = 2**dense_num, activation = 'relu')(x)\n",
    "            x = Dense(units = num_category, activation = 'softmax')(x)\n",
    "            model = Model(inputs = inputs, outputs = x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def opt_cnn(trial):\n",
    "    # Opt params -----------------------\n",
    "    # Categorical parameter\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 2)\n",
    "    dense_num = trial.suggest_int('dense_num', 3, 7)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(f'num_filter_{i}', 7, 10, 1)) for i in range(num_layer)]\n",
    "    size_filters = [int(trial.suggest_discrete_uniform(f'size_filter_{i}', 3, 5, 2)) for i in range(num_layer)]\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 5)\n",
    "    # Model Compiler -----------------------\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    decay = trial.suggest_loguniform('decay', 1e-6, 1e-3)\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = trial.suggest_discrete_uniform('dropout_rate_in', 0.0, 0.5, 0.1)\n",
    "    dropout_rate_out = trial.suggest_discrete_uniform('dropout_rate_out', 0.0, 0.5, 0.1)\n",
    "    momentum = trial.suggest_discrete_uniform('momentum', 0.0, 1.0, 0.1)\n",
    "    # categorical parameter\n",
    "#    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"rmsprop\", \"adam\"])\n",
    "    padding = trial.suggest_categorical('padding', ['same', 'valid'])\n",
    "    # compile model-------------------\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True)\n",
    "#    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "    # For CPU run ------------------\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train Model ----------------------------------\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 0)\n",
    "    pr_cb = integration.TFKerasPruningCallback(trial, 'val_loss')\n",
    "    cbs = [es_cb, pr_cb]\n",
    "    loss_list, acc_list = [], []\n",
    "    for inner_cv in range(0, cvs):\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = data_splitter_cv(train_files, X_outer_train, Y_outer_train, inner_cv, val_train_region, val_train_point)\n",
    "        hist = model.fit(\n",
    "            train_datagen.flow(X_inner_train, Y_inner_train, batch_size = (2**batch_size) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_inner_val, Y_inner_val),\n",
    "            callbacks = cbs,\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False)\n",
    "        loss_list += [model.evaluate(X_inner_val, Y_inner_val)[0]]\n",
    "        acc_list += [model.evaluate(X_inner_val, Y_inner_val)[1]]\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    eval_loss = np.mean(loss_list)\n",
    "    eval_acc = np.mean(acc_list)\n",
    "    return eval_loss\n",
    "\n",
    "def opt_lgb(trial):\n",
    "    \n",
    "    param_grid_lgb = {\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 3, 30),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"random_state\": SEED\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**param_grid_lgb)\n",
    "    \n",
    "    # 10-Fold CV / Accuracy でモデルを評価する\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = cross_validate(model, X=X_outer_train, y=Y_outer_train, cv=kf)\n",
    "    # 最小化なので 1.0 からスコアを引く\n",
    "    return scores['test_score'].mean()\n",
    "#     return lgb.score(X_outer_val, Y_outer_val)\n",
    "\n",
    "\n",
    "def mean_params_calc(param_names):\n",
    "    dict = {}\n",
    "    categoricals = ['padding']\n",
    "    for param_name in param_names:\n",
    "        data_num = 0\n",
    "        if param_name not in categoricals:\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    try:\n",
    "                        dict[param_name] += data[param_name]\n",
    "                    except:\n",
    "                        dict[param_name] = data[param_name]\n",
    "                    data_num = data_num + 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = dict[param_name]/data_num\n",
    "        else:\n",
    "            categorical_list = []\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    categorical_list = categorical_list + [data[param_name]]\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = stats.mode(categorical_list)[0][0]\n",
    "    return dict\n",
    "\n",
    "\n",
    "def cv_result_imgs_generator(model, history):\n",
    "    # Visualize Loss Results ----------------------------\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\", marker=\"o\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\", marker=\"o\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(color='gray', alpha=0.2)\n",
    "    plt.savefig('./img_loss/' + str(outer_cv) + '_loss.jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generalization_result_imgs_generator(name, Y_val_pred, Y_val_all):\n",
    "    # Evaluate test data -----------------------\n",
    "    plt.figure()\n",
    "    plt.scatter(Y_val_all, Y_val_pred, s=3, alpha=0.5)\n",
    "    plt.xlim(min([np.min(Y_val_all), np.min(Y_val_pred)]), max([np.max(Y_val_all),np.max(Y_val_pred)]))\n",
    "    plt.xlabel(\"obs\")\n",
    "    plt.ylabel(\"pred\")\n",
    "    x = np.linspace(min([np.min(Y_val_all), np.min(Y_val_pred)]), max([np.max(Y_val_all),np.max(Y_val_pred)]),100)\n",
    "    y = x\n",
    "    plt.plot(x, y, \"r-\")\n",
    "    plt.savefig('./img_loss/' + name + '_scatter_test.jpg')\n",
    "    plt.close()\n",
    "    \n",
    "def region_image_generator(point, region):\n",
    "    data_num = int(len(imgfiles)/28)\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(point[:data_num][:,0],point[:data_num][:,1], marker='o', s=5, color=cmap(region))\n",
    "    ax.set_title(\"Region in Japan\")\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")\n",
    "    fig.savefig('./region_separate.png')\n",
    "    \n",
    "    \n",
    "def make_df(X):\n",
    "    return pd.DataFrame(X)\n",
    "\n",
    "def data_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}.pkl')\n",
    "    else:\n",
    "        trial = int(len(imgfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "            Y_28 = file_split[5].split(\".\")[0]\n",
    "            point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "            filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "            X.append(X_28[box*28: (box+1)*28])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}.pkl'))\n",
    "        \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "\n",
    "# Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "\n",
    "# Data converter ----------------------------------------------\n",
    "# X->説明変数, Y->目的変数, point->緯度経度, region->領域を10分割した時の分割区間名\n",
    "filenames, X, Y, point, region = data_import()\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "num_category = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_cv_0_processing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-12 16:38:52,136]\u001b[0m Finished trial#0 resulted in value: 0.2651773048400108. Current best value is 0.2651773048400108 with parameters: {'num_leaves': 9, 'learning_rate': 4.9753119101948975e-08, 'max_depth': 7}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:38:56,681]\u001b[0m Finished trial#1 resulted in value: 0.7308060748843006. Current best value is 0.7308060748843006 with parameters: {'num_leaves': 11, 'learning_rate': 0.18896033331462503, 'max_depth': 8}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:02,728]\u001b[0m Finished trial#2 resulted in value: 0.2651773048400108. Current best value is 0.7308060748843006 with parameters: {'num_leaves': 11, 'learning_rate': 0.18896033331462503, 'max_depth': 8}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:06,290]\u001b[0m Finished trial#3 resulted in value: 0.2663249054073254. Current best value is 0.7308060748843006 with parameters: {'num_leaves': 11, 'learning_rate': 0.18896033331462503, 'max_depth': 8}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:12,290]\u001b[0m Finished trial#4 resulted in value: 0.2651773048400108. Current best value is 0.7308060748843006 with parameters: {'num_leaves': 11, 'learning_rate': 0.18896033331462503, 'max_depth': 8}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:18,681]\u001b[0m Finished trial#5 resulted in value: 0.7326038807468734. Current best value is 0.7326038807468734 with parameters: {'num_leaves': 20, 'learning_rate': 0.0875024761627558, 'max_depth': 7}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:27,337]\u001b[0m Finished trial#6 resulted in value: 0.7353956943378133. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:30,322]\u001b[0m Finished trial#7 resulted in value: 0.2651773048400108. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:33,665]\u001b[0m Finished trial#8 resulted in value: 0.6910216008860012. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:39,556]\u001b[0m Finished trial#9 resulted in value: 0.7048695967972154. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:47,712]\u001b[0m Finished trial#10 resulted in value: 0.2651773048400108. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:39:53,558]\u001b[0m Finished trial#11 resulted in value: 0.36157780830027575. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:40:00,993]\u001b[0m Finished trial#12 resulted in value: 0.607551340641467. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:40:04,681]\u001b[0m Finished trial#13 resulted in value: 0.5685318605276939. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:40:11,025]\u001b[0m Finished trial#14 resulted in value: 0.41861577544923023. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n",
      "\u001b[32m[I 2020-11-12 16:40:18,884]\u001b[0m Finished trial#15 resulted in value: 0.2651773048400108. Current best value is 0.7353956943378133 with parameters: {'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': 30, 'learning_rate': 0.20365043745312883, 'max_depth': 14}\n",
      "0.7353956943378133\n",
      "0.7593135816955119\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d4bcdeb3edc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mY_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_outer_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_val_pred' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d4bcdeb3edc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mY_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_outer_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mY_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgb_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_outer_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mY_val_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_outer_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "if model == \"cnn\":\n",
    "    # Data standardizing ----------------------------------------------\n",
    "    X_train_mean_lis, X_train_std_lis = [], []\n",
    "    X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    for i in range(X_train.shape[3]):\n",
    "        X_train_mean_lis.append(X_train[:,:,:,i].mean())\n",
    "        X_train_std_lis.append(X_train[:,:,:,i].std())\n",
    "        X_train[:,:,:,i] = (X_train[:,:,:,i] - X_train_mean_lis[i]) / X_train_std_lis[i]\n",
    "        X_test[:,:,:,i] = (X_test[:,:,:,i] - X_train_mean_lis[i]) / X_train_std_lis[i]\n",
    "    \n",
    "    timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "    time_path      =  os.path.join(result_path, timename, \"outer_cv_times\")\n",
    "    # dir generation\n",
    "    dir_generator(result_path)\n",
    "    # Chenge current directry\n",
    "    os.mkdir(os.path.join(result_path, timename))\n",
    "    os.chdir(os.path.join(result_path, timename))\n",
    "    dir_generator(model_path)\n",
    "    dir_generator(\"./results/\")\n",
    "    dir_generator(\"./img_loss/\")\n",
    "    dir_generator(\"./model/\")\n",
    "    dir_generator(\"./weights/\")\n",
    "    dir_generator(\"./logs/\")\n",
    "    dir_generator(\"./outer_cv_times/\")\n",
    "\n",
    "\n",
    "    # Saving region_separate_map\n",
    "    region_image_generator(point, region)\n",
    "    train_start = datetime.datetime.now()\n",
    "    # Train Model ----------------------------------\n",
    "    # CV start ------------------------------------------------------------\n",
    "    for outer_cv in range(cvs):\n",
    "        outer_start = datetime.datetime.now()\n",
    "        print(f'outer_cv_{outer_cv}_processing....')\n",
    "        # Data Loader-------------------------------------\n",
    "        train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = data_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "        train_datagen = ImageDataGenerator(\n",
    "    #         rotation_range = 360,\n",
    "            horizontal_flip = True,\n",
    "            vertical_flip = True\n",
    "        )\n",
    "        val_train_region = KMeans(n_clusters = cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "        # Bayesian optimization -------------------------------------\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(opt_cnn, n_trials = ntrials)\n",
    "        # Best_model_training ---------------------------------------\n",
    "        num_filters = [int(study.best_params[f'num_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "        size_filters = [int(study.best_params[f'size_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "        model = create_model(image_shape, int(study.best_params['num_layer']), study.best_params['padding'], int(study.best_params['dense_num']), num_filters, size_filters, study.best_params['dropout_rate_in'], study.best_params['dropout_rate_out'])\n",
    "        sgd = optimizers.SGD(lr = study.best_params['learning_rate'], decay = study.best_params['decay'], momentum = study.best_params['momentum'], nesterov = True, clipvalue = 1.0)\n",
    "        model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            train_datagen.flow(X_outer_train, Y_outer_train, batch_size = 2**int(study.best_params['batch_size']) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_outer_val, Y_outer_val),\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False\n",
    "            )\n",
    "        try:\n",
    "            best_params.append(study.best_params)\n",
    "        except:\n",
    "            best_params = [study.best_params]\n",
    "        try:\n",
    "            val_pred_files = np.concatenate((val_pred_files, val_files), axis=0)\n",
    "        except:\n",
    "            val_pred_files = val_files\n",
    "        try:\n",
    "            Y_val_pred = np.concatenate((Y_val_pred, model.predict(X_outer_val).argmax(axis=1)), axis=0)\n",
    "        except:\n",
    "            Y_val_pred = np.array(model.predict(X_outer_val).argmax(axis=1))\n",
    "        try:\n",
    "            Y_val_obs = np.concatenate((Y_val_obs, Y_outer_val), axis=0)\n",
    "        except:\n",
    "            Y_val_obs = Y_outer_val\n",
    "        try:\n",
    "            Y_val_smx = np.concatenate((Y_val_smx, model.predict(X_outer_val)),axis=0)\n",
    "        except:\n",
    "            Y_val_smx = model.predict(X_outer_val)\n",
    "        cv_result_imgs_generator(model, history)\n",
    "        print(\"accuracy is\", model.evaluate(X_outer_val, Y_outer_val)[1])\n",
    "        #compare_TV(history, outer_cv)\n",
    "        del model\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        outer_end = datetime.datetime.now()\n",
    "        spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "        pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "    train_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))\n",
    "\n",
    "\n",
    "elif model == \"lgb\":\n",
    "    timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "    time_path      =  os.path.join(result_path, timename, \"outer_cv_times\")\n",
    "    # dir generation\n",
    "    dir_generator(result_path)\n",
    "    # Chenge current directry\n",
    "    os.mkdir(os.path.join(result_path, timename))\n",
    "    os.chdir(os.path.join(result_path, timename))\n",
    "    dir_generator(model_path)\n",
    "    dir_generator(\"./results/\")\n",
    "    dir_generator(\"./img_loss/\")\n",
    "    dir_generator(\"./model/\")\n",
    "    dir_generator(\"./weights/\")\n",
    "    dir_generator(\"./logs/\")\n",
    "    dir_generator(\"./outer_cv_times/\")\n",
    "#     中心の一点だけ使います\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_XY.pkl'):\n",
    "        df_XY = pkl_loader(data_path + f'df_{N}x{N}_XY.pkl')\n",
    "    else:\n",
    "        data_num = int(len(imgfiles)/28)\n",
    "        df_XY = pd.DataFrame(np.zeros((data_num,29)))\n",
    "        for i in range(data_num):\n",
    "            df_XY.iloc[i,0] = Y[i]\n",
    "            for j in range(28):\n",
    "                df_XY.iloc[i, j+1] = X[i][1][1][j]\n",
    "        pkl_saver(df_XY, os.path.join(data_path, f'df_{N}x{N}_XY.pkl'))\n",
    "\n",
    "    Y, X = df_XY[0], df_XY.iloc[:, 1:]\n",
    "    X_train_mean_lis = []\n",
    "    X_train_std_lis = []\n",
    "    X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, 0, region_train, train_point)\n",
    "    \n",
    "    for outer_cv in range(cvs):\n",
    "        outer_start = datetime.datetime.now()\n",
    "        print(f'outer_cv_{outer_cv}_processing....')\n",
    "        # Data Loader-------------------------------------\n",
    "        train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, 0, region_train, train_point)\n",
    "        val_train_region = KMeans(n_clusters = cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "\n",
    "\n",
    "    #     for inner_cv in range(0, cvs):\n",
    "    #         _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = data_splitter_cv(train_files, X_outer_train, Y_outer_train, inner_cv, val_train_region, val_train_point)\n",
    "    #         lgb = LGBMClassifier(random_state=SEED)\n",
    "    #         lgb.fit(X_inner_train, _inner_train)\n",
    "    #         print(f'inner_cv is {inner_cv}')\n",
    "    #         print(f'accuracy of train set: {lgb.score(X_inner_train, Y_inner_train)}')\n",
    "    #         print(f'accuracy of train set: {lgb.score(X_inner_val, Y_inner_val)}')\n",
    "    #         print()\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(opt_lgb, n_trials=ntrials)\n",
    "        print(study.best_params)\n",
    "        print(study.best_value)\n",
    "        lgb_best_param = study.best_params\n",
    "\n",
    "        lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "        lgb_best.fit(X_train, Y_train)\n",
    "        pred = pd.DataFrame(lgb_best.predict(X_test))\n",
    "        print((np.array(pred[0]).astype(int) == Y_test.values.astype(int)).sum() / len(Y_test))\n",
    "        \n",
    "        try:\n",
    "            Y_val_pred = np.concatenate((Y_val_pred, lgb_best.predict(X_outer_val).argmax(axis=1)), axis=0)\n",
    "        except:\n",
    "            Y_val_pred = np.array(lgb_best.predict(X_outer_val).argmax(axis=1))\n",
    "        try:\n",
    "            Y_val_obs = np.concatenate((Y_val_obs, Y_outer_val), axis=0)\n",
    "        except:\n",
    "            Y_val_obs = Y_outer_val\n",
    "        try:\n",
    "            Y_val_smx = np.concatenate((Y_val_smx, lgb_best.predict(X_outer_val)),axis=0)\n",
    "        except:\n",
    "            Y_val_smx = lgb_best.predict(X_outer_val)\n",
    "\n",
    "        outer_end = datetime.datetime.now()\n",
    "        spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "        pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "    train_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))\n",
    "    \n",
    "else:\n",
    "    print(\"modelを正しく選択してください\")\n",
    "\n",
    "# code save\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "shutil.copy(r\"C:\\Users\\GE\\Dropbox\\Kairo\\code\\3_study_code\\LULC_code\\LULC_CNN_lightGBM.ipynb\", \"./code/LULC_CNN_lightGBM.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Save CV_Result -------------------------------------------------\n",
    "    # generalization_result_imgs_generator('val', Y_val_pred, Y_val_obs)\n",
    "    np.savetxt('Y_val_smx.txt', Y_val_smx)\n",
    "    param_names = best_params[list(map(len, best_params)).index(max(list(map(len, best_params))))].keys()\n",
    "    best_params_dict = mean_params_calc(param_names)\n",
    "    pkl_saver(best_params, 'best_params_list.binaryfile')\n",
    "    pkl_saver(best_params_dict, 'best_params.binaryfile')\n",
    "    best_params_dict = pkl_loader('best_params.binaryfile')\n",
    "\n",
    "    # Save CV_Result to csv -------------------------------------------------\n",
    "    results = [val_pred_files, Y_val_obs, Y_val_pred, Y_val_smx]\n",
    "    pkl_saver(results, './results/results.pkl')\n",
    "    results_csv = np.concatenate([pd.DataFrame(val_pred_files),pd.DataFrame(Y_val_obs), pd.DataFrame(Y_val_pred), pd.DataFrame(Y_val_smx)], 1)\n",
    "    results_csv = pd.DataFrame(results_csv)\n",
    "    columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "    results_csv.columns=columns\n",
    "    results_csv.to_csv('./results/results_val.csv')\n",
    "    labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "    cf_metr = confusion_matrix(Y_val_obs.astype(int), Y_val_pred)\n",
    "    cf_metr = pd.DataFrame(cf_metr)\n",
    "    cf_metr.columns=labels\n",
    "    cf_metr.index=labels\n",
    "    cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "    res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "    with open('./results/result_summary_val.txt','w') as f:\n",
    "        f.write(res_smr)\n",
    "\n",
    "    # Best Model Training -----------------------------------------------\n",
    "    # Int parameter\n",
    "    num_layer = int(best_params_dict['num_layer'])\n",
    "    num_filters = [int(best_params_dict['num_filter_' + str(i)]) for i in range(num_layer)]\n",
    "    size_filters = [int(best_params_dict['size_filter_' + str(i)]) for i in range(num_layer)]\n",
    "    dense_num = int(best_params_dict['dense_num'])\n",
    "    batch_size = int(best_params_dict['batch_size'])\n",
    "    # Uniform parameter\n",
    "    # Loguniform parameter\n",
    "    lr = best_params_dict['learning_rate']\n",
    "    decay = best_params_dict['decay']\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = best_params_dict['dropout_rate_in']\n",
    "    dropout_rate_out = best_params_dict['dropout_rate_out']\n",
    "    momentum = best_params_dict['momentum']\n",
    "    # Categorical parameter\n",
    "    padding = best_params_dict['padding']\n",
    "\n",
    "\n",
    "    # Model Checkpoint ------------------\n",
    "    cp_cb = ModelCheckpoint(\n",
    "        './weights/best_weights.hdf5',\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = 'auto')\n",
    "    # Logging ----------------------------------------\n",
    "    log_dir = os.path.join('./logs/')\n",
    "    tb_cb = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = int(best_epochs/10), verbose = 1)\n",
    "\n",
    "    cbs = [cp_cb, tb_cb, es_cb]\n",
    "    # Train Best_Model ----------------------------------\n",
    "    # For CPU run ------------------\n",
    "    best_model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "\n",
    "    best_model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy')\n",
    "    hist = best_model.fit(\n",
    "        train_datagen.flow(X_train, Y_train, batch_size = (2**batch_size) * gpus),\n",
    "        epochs = best_epochs,\n",
    "        callbacks = cbs,\n",
    "        shuffle = True,\n",
    "        verbose = 1,\n",
    "        initial_epoch = 0,\n",
    "        use_multiprocessing = False)\n",
    "\n",
    "    # Save Model -----------------------------------\n",
    "    best_model.save('./model/best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 2, 2, ..., 1, 8, 2]), array([5, 2, 2, ..., 1, 8, 2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred[0]).astype(int), Y_test.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7374596655910824\n"
     ]
    }
   ],
   "source": [
    "print((np.array(pred[0]).astype(int) == Y_test.values.astype(int)).sum() / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model == \"cnn\":\n",
    "    # ロードだけでも動くように変数の再定義=============================================\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "    # Data Loader ------------------------------\n",
    "    best_model = load_model(\"./model/best_model.hdf5\")\n",
    "    df = pkl_loader(os.path.join(data_path, f'df_{N}x{N}.pkl'))\n",
    "    # Data converter ----------------------------------------------\n",
    "    filenames, X, Y, point, region = df[0], df[1], df[2], df[3], df[4]\n",
    "    image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "    num_category = len(np.unique(Y))\n",
    "    # Data splitting ----------------------------------------------\n",
    "    X_train_mean_lis = []\n",
    "    X_train_std_lis = []\n",
    "    X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    for i in range(X_train.shape[3]):\n",
    "        X_train_mean_lis.append(X_train[:,:,:,i].mean())\n",
    "        X_train_std_lis.append(X_train[:,:,:,i].std())\n",
    "        X_train[:,:,:,i] = (X_train[:,:,:,i] - X_train_mean_lis[i])/X_train_std_lis[i]\n",
    "        X_test[:,:,:,i] = (X_test[:,:,:,i] - X_train_mean_lis[i])/X_train_std_lis[i]\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "    # ==============================================================================\n",
    "\n",
    "\n",
    "    Y_test_pred = [ np.array(best_model.predict(X_test).argmax(axis=1))]\n",
    "    np.savetxt('y_test_pred.txt', Y_test_pred)\n",
    "    with open(\"best_model_summary.txt\", \"w\") as fp:\n",
    "        best_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "\n",
    "\n",
    "    results = [Y_files, Y_test, Y_test_pred]\n",
    "    pkl_saver(results, './results/results.pkl')\n",
    "    results_csv = np.concatenate([pd.DataFrame(Y_files),pd.DataFrame(Y_test), pd.DataFrame(Y_test_pred[0])], 1)\n",
    "    results_csv = pd.DataFrame(results_csv)\n",
    "    columns = [\"name\", \"obs\", \"pred\"]\n",
    "    results_csv.columns=columns\n",
    "    results_csv.to_csv('./results/results_test.csv')\n",
    "    labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "    cf_metr = confusion_matrix(Y_test.astype(int), Y_test_pred[0])\n",
    "    cf_metr = pd.DataFrame(cf_metr)\n",
    "    cf_metr.columns=labels\n",
    "    cf_metr.index=labels\n",
    "    cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "    test_smr = classification_report(list(np.array(Y_test).astype(int)), list(Y_test_pred[0].astype(int)), target_names = labels, labels = np.array(range(len(labels))))\n",
    "    with open('./results/result_summary_test.txt','w') as f:\n",
    "        f.write(test_smr)\n",
    "\n",
    "        \n",
    "elif model == \"lgb\":\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_XY.pkl'):\n",
    "        df_XY = pkl_loader(data_path + f'df_{N}x{N}_XY.pkl')\n",
    "    else:\n",
    "        data_num = int(len(imgfiles)/28)\n",
    "        df_XY = pd.DataFrame(np.zeros((data_num,29)))\n",
    "        for i in range(data_num):\n",
    "            df_XY.iloc[i,0] = Y[i]\n",
    "            for j in range(28):\n",
    "                df_XY.iloc[i, j+1] = X[i][1][1][j]\n",
    "        pkl_saver(df_XY, os.path.join(data_path, f'df_{N}x{N}_XY.pkl'))\n",
    "\n",
    "    Y, X = df_XY[0], df_XY.iloc[:, 1:]\n",
    "    X_train_mean_lis = []\n",
    "    X_train_std_lis = []\n",
    "    X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, 0, region_train, train_point)\n",
    "\n",
    "    \n",
    "print('finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code save\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "shutil.copy(r\"C:\\Users\\GE\\Dropbox\\Kairo\\code\\3_study_code\\LULC_code\\LULC_CNN_model.ipynb\", \"./code/LULC_CNN_model.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(Y_test_pred).astype(int) == Y_test.astype(int)).sum() / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
