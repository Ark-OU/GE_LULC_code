{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変数定義\n",
    "\n",
    "4 seasons * 7 bandsのデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define vars ---------------------------------------------\n",
    "N = 3\n",
    "under = 90\n",
    "gpu_list = ['/gpu:0', '/gpu:1']\n",
    "SEED = 31\n",
    "\n",
    "# 共通params ---------------------------------------------------\n",
    "n_trials  = 2**3          # ベイズ最適化回数\n",
    "outer_cvs = 10\n",
    "inner_cvs = 5\n",
    "\n",
    "# CNN Training params ------------------------------------------\n",
    "train_epochs = 2**2      # エポック数\n",
    "best_epochs = 2**5       # 最終モデル決定用のエポック数\n",
    "early_stopping = 2**3    # \n",
    "\n",
    "# LightGBM params -----------------------------------------------\n",
    "lgb_boosting_type = 'gbdt'\n",
    "\n",
    "\n",
    "import os, zipfile, io, re\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# Utils -----------------------\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, zipfile, io, re\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import ipynb_path\n",
    "from math import sqrt\n",
    "import tifffile\n",
    "# Machine Learning ---------------\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from optuna import integration\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "# Keras, TensorFlow ---------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization, Convolution2D, Input\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 31\n",
    "np.random.seed(SEED)\n",
    "gpus = len(gpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# define functions\n",
    "def pkl_saver(object, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as web:\n",
    "        pickle.dump(object , web)\n",
    "\n",
    "def pkl_loader(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as web:\n",
    "        data = pickle.load(web)\n",
    "    return data\n",
    "\n",
    "def dir_generator(dir_path):\n",
    "    if os.path.exists(dir_path) == False:\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "def data_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}.pkl')\n",
    "    else:\n",
    "        trial = int(len(imgfiles)/28)\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "            Y_28 = file_split[5].split(\".\")[0]\n",
    "            point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "            filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "            X.append(X_28[box*28: (box+1)*28])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        # label encorder===========================================\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for i in range(len(labels)):\n",
    "            Y[Y==labels[i]] = int(i)\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}.pkl'))\n",
    "        \n",
    "    return df[0], df[1], df[2], df[3], df[4]\n",
    "def test_import():\n",
    "    if os.path.exists(data_path + f'df_{N}x{N}_testset.pkl'):\n",
    "        df = pkl_loader(data_path + f'df_{N}x{N}_testset.pkl')\n",
    "    else:\n",
    "        trial = int(len(testfiles)/28)\n",
    "    #     trial = 100\n",
    "        X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "        Y = [] # Y: 目的変数\n",
    "        point = [] # point: 緯度経度\n",
    "        X_28 = []\n",
    "        Y_28 = 0\n",
    "        point_28 = []\n",
    "        filenames = []\n",
    "        max_light = 0\n",
    "        print('inputdata_processing...')\n",
    "\n",
    "        labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land' ]\n",
    "        for box in tqdm(range(trial)):\n",
    "            for imgfile in testfiles[box*28: (box+1)*28]:\n",
    "                # ZIPから画像読み込み\n",
    "                image = tifffile.imread(imgfile)\n",
    "        #         print(image.shape)\n",
    "                file = os.path.basename(imgfile)\n",
    "                file_split = [i for i in file.split('_')]\n",
    "                X_28.append(image)\n",
    "\n",
    "            Y_28 = int(file_split[4].split(\".\")[0]) - 1\n",
    "    #         print(file_split, Y_28)\n",
    "            point_28 = [float(file_split[0]), float(file_split[1])]\n",
    "            filenames.append(f\"test_{file_split[0]}_{file_split[1]}_{labels[Y_28]}\")\n",
    "            X.append(X_28[box*28: (box+1)*28])\n",
    "            Y.append(Y_28)\n",
    "            point.append(point_28)\n",
    "        del X_28, Y_28, point_28\n",
    "        X = np.asarray(X)\n",
    "        print(X.shape)\n",
    "        X = X.transpose(0,2,3,1)\n",
    "        print(X.shape)\n",
    "        Y = np.array(Y)\n",
    "        filenames = np.array(filenames)\n",
    "        point = np.array(point)\n",
    "        region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(point).labels_\n",
    "        df = [filenames, X, Y, point, region]\n",
    "        pkl_saver(df, os.path.join(data_path, f'df_{N}x{N}_testset.pkl'))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def data_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_test = Y[test_index]\n",
    "    X_train = X[train_index]\n",
    "    Y_train = Y[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "# Loss Definition ----------------------------------\n",
    "def opt_cnn(trial):\n",
    "    # Opt params -----------------------\n",
    "    # Categorical parameter\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 2)\n",
    "    dense_num = trial.suggest_int('dense_num', 3, 7)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(f'num_filter_{i}', 7, 10, 1)) for i in range(num_layer)]\n",
    "    size_filters = [int(trial.suggest_discrete_uniform(f'size_filter_{i}', 3, 5, 2)) for i in range(num_layer)]\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 5)\n",
    "    # Model Compiler -----------------------\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    decay = trial.suggest_loguniform('decay', 1e-6, 1e-3)\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = trial.suggest_discrete_uniform('dropout_rate_in', 0.0, 0.5, 0.1)\n",
    "    dropout_rate_out = trial.suggest_discrete_uniform('dropout_rate_out', 0.0, 0.5, 0.1)\n",
    "    momentum = trial.suggest_discrete_uniform('momentum', 0.0, 1.0, 0.1)\n",
    "    # categorical parameter\n",
    "#    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"rmsprop\", \"adam\"])\n",
    "    padding = trial.suggest_categorical('padding', ['same', 'valid'])\n",
    "    # compile model-------------------\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True)\n",
    "#    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "    # For CPU run ------------------\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train Model ----------------------------------\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 0)\n",
    "    pr_cb = integration.TFKerasPruningCallback(trial, 'val_loss')\n",
    "    cbs = [es_cb, pr_cb]\n",
    "    loss_list, acc_list = [], []\n",
    "    for inner_cv in range(0, inner_cvs):\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = data_splitter_cv(train_files, X_outer_train, Y_outer_train, inner_cv, val_train_region, val_train_point)\n",
    "        hist = model.fit(\n",
    "            train_datagen.flow(X_inner_train, Y_inner_train, batch_size = (2**batch_size) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_inner_val, Y_inner_val),\n",
    "            callbacks = cbs,\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False)\n",
    "        loss_list += [model.evaluate(X_inner_val, Y_inner_val)[0]]\n",
    "        acc_list += [model.evaluate(X_inner_val, Y_inner_val)[1]]\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    eval_loss = np.mean(loss_list)\n",
    "    eval_acc = np.mean(acc_list)\n",
    "    return eval_loss\n",
    "\n",
    "def create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out):\n",
    "    inputs = Input(image_shape)\n",
    "    for d in gpu_list:\n",
    "        with tf.device(d):\n",
    "            x = Dropout(dropout_rate_in)(inputs)\n",
    "            x = Convolution2D(filters = 2**num_filters[0], kernel_size = (size_filters[0],size_filters[0]), padding = 'same', activation = 'relu')(x)\n",
    "            for i in range(1, num_layer):\n",
    "                x = Convolution2D(filters = 2**num_filters[i],\n",
    "                                  kernel_size = (size_filters[i], size_filters[i]),\n",
    "                                  padding = padding,\n",
    "                                  activation = 'relu')(x)\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dropout(dropout_rate_out)(x)\n",
    "            x = Dense(units = 2**dense_num, activation = 'relu')(x)\n",
    "            x = Dense(units = num_category, activation = 'softmax')(x)\n",
    "            model = Model(inputs = inputs, outputs = x)\n",
    "    return model\n",
    "\n",
    "def mean_params_calc(param_names):\n",
    "    dict = {}\n",
    "    categoricals = ['padding']\n",
    "    for param_name in param_names:\n",
    "        data_num = 0\n",
    "        if param_name not in categoricals:\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    try:\n",
    "                        dict[param_name] += data[param_name]\n",
    "                    except:\n",
    "                        dict[param_name] = data[param_name]\n",
    "                    data_num = data_num + 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = dict[param_name]/data_num\n",
    "        else:\n",
    "            categorical_list = []\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    categorical_list = categorical_list + [data[param_name]]\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = stats.mode(categorical_list)[0][0]\n",
    "    return dict\n",
    "\n",
    "def cv_result_imgs_generator(model, history):\n",
    "    # Visualize Loss Results ----------------------------\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\", marker=\"o\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\", marker=\"o\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(color='gray', alpha=0.2)\n",
    "    plt.savefig('./img_loss/' + str(outer_cv) + '_loss.jpg')\n",
    "    plt.close()\n",
    "\n",
    "def region_image_generator(point, region):\n",
    "    data_num = int(len(imgfiles)/28)\n",
    "    cmap = plt.get_cmap(\"tab10\")\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.scatter(point[:data_num][:,0],point[:data_num][:,1], marker='o', s=5, color=cmap(region))\n",
    "    ax.set_title(\"Region in Japan\")\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")\n",
    "    fig.savefig('./region_separate.png')\n",
    "\n",
    "def make_dirs(model_name):\n",
    "    base_path = os.path.join(result_path , model_name)\n",
    "    # dir generation\n",
    "    dir_generator(base_path)\n",
    "    # Chenge current directry\n",
    "    os.mkdir(os.path.join(base_path, timename))\n",
    "    os.chdir(os.path.join(base_path, timename))\n",
    "    dir_generator(model_path)\n",
    "    dir_generator(\"./results/\")\n",
    "    dir_generator(\"./img_loss/\")\n",
    "    dir_generator(\"./model/\")\n",
    "    dir_generator(\"./weights/\")\n",
    "    dir_generator(\"./logs/\")\n",
    "    dir_generator(\"./outer_cv_times/\")\n",
    "\n",
    "def time_printer(start_time):\n",
    "    end_time = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {end_time - start_time} seconds.\"\n",
    "    \n",
    "# LightGBM ----------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "def opt_lgb(trial):\n",
    "    if lgb_boosting_type == \"gbdt\":\n",
    "        param_grid_lgb = {\n",
    "    #         \"device\": \"gpu\",\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 15),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    elif lgb_boosting_type ==\"rf\":\n",
    "        param_grid_lgb = {\n",
    "            'boosting_type': lgb_boosting_type,\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 15, 35),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 15),\n",
    "    #         'n_estimators': trial.suggest_int(\"n_estimators\", 70, 120),\n",
    "            'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 0.3),\n",
    "            \"random_state\": SEED\n",
    "        }\n",
    "    \n",
    "    scores = []\n",
    "    for inner_cv in range(inner_cvs):\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = lgb_splitter_cv(train_files, X_outer_train, Y_outer_train, outer_cv, val_train_region, val_train_point)\n",
    "\n",
    "        model = LGBMClassifier(**param_grid_lgb)\n",
    "        model.fit(X_inner_train, Y_inner_train)\n",
    "        \n",
    "        scores.append(model.score(X_inner_val, Y_inner_val))\n",
    "    \n",
    "#     print('mean of inner_val_scores is ', np.mean(scores))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def lgb_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = np.array(X)[test_index]\n",
    "    Y_test = np.array(Y)[test_index]\n",
    "    X_train = np.array(X)[train_index]\n",
    "    Y_train = np.array(Y)[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    X_train, X_test, Y_train, Y_test = make_df(X_train), make_df(X_test), make_df(Y_train), make_df(Y_test)\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "def make_df(X):\n",
    "    return pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data import ------------------------------------------------------------\n",
    "N=3\n",
    "standarization = [\"normalization\", \"Zscore\", \"normal\"] # pklを作るまではCodeの書き換えも必要だよ！\n",
    "standarization_num= 2\n",
    "# Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, \"CNN\", timename, \"outer_cv_times\")\n",
    "make_dirs(\"CNN\")\n",
    "\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# Data converter ----------------------------------------------\n",
    "# X->説明変数, Y->目的変数, point->緯度経度, region->領域を10分割した時の分割区間名\n",
    "filenames, X, Y, point, region = data_import()\n",
    "X = X.astype(np.float64)\n",
    "Y = Y.astype(np.float64)\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "num_category = len(np.unique(Y))\n",
    "\n",
    "X_files, X_train, Y_train, train_point, region_train = data_import()\n",
    "X_train = X_train.astype(np.float64)\n",
    "\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl'):\n",
    "    df_XY = pkl_loader(data_path + f'df_{N}x{N}_{standarization[standarization_num]}.pkl')\n",
    "else:\n",
    "    X_train_zeros = np.zeros((X_train.shape[0], N*N*28))\n",
    "    for i in range(len(X_train)):\n",
    "        for k in range(3):\n",
    "            for l in range(3):\n",
    "                for j in range(28):\n",
    "                    X_train_zeros[i][j+k+l] = X_train[i][k][l][j]\n",
    "    X_train = X_train_zeros\n",
    "#     for k in range(3):\n",
    "#         for l in range(3):\n",
    "#             for j in range(28):\n",
    "#                 X_min = X_train[j+k+l].min()\n",
    "#                 X_max = X_train[j+k+l].max()\n",
    "#                 if X_max - X_min == 0:\n",
    "#                     continue\n",
    "#                 X_train[:,j+k+l] = (X_train[:,j+k+l] - X_min) / (X_max - X_min)\n",
    "\n",
    "    df_XY = pd.concat([make_df(Y_train), make_df(X_train)], axis=1)\n",
    "    pkl_saver(df_XY, os.path.join(data_path, f'df_{N}x{N}_{standarization[standarization_num]}.pkl'))\n",
    "    \n",
    "\n",
    "Y_files, X_test, Y_test, test_point, region_test = test_import()\n",
    "X_test = X_test.astype(np.float64)\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'):\n",
    "    df_test = pkl_loader(data_path + f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl')\n",
    "else:\n",
    "    X_test_zeros = np.zeros((X_test.shape[0], N*N*28))\n",
    "    for i in range(len(X_test)):\n",
    "        for k in range(3):\n",
    "            for l in range(3):\n",
    "                for j in range(28):\n",
    "                    X_test_zeros[i][j+k+l] = X_test[i][k][l][j]\n",
    "    X_test = X_test_zeros\n",
    "#     for k in range(3):\n",
    "#         for l in range(3):\n",
    "#             for j in range(28):\n",
    "#                 X_min = X_test[j+k+l].min()\n",
    "#                 X_max = X_test[j+k+l].max()\n",
    "#                 if X_max - X_min == 0:\n",
    "#                     continue\n",
    "#                 X_test[:,j+k+l] = (X_test[:,j+k+l] - X_min) / (X_max - X_min)\n",
    "    df_test = pd.concat([make_df(Y_test), make_df(X_test)], axis=1)\n",
    "    pkl_saver(df_test, os.path.join(data_path, f'df_{N}x{N}_test_{standarization[standarization_num]}.pkl'))\n",
    "\n",
    "# Data converter ----------------------------------------------\n",
    "X_train = df_XY.iloc[:, 1:]\n",
    "Y_train = make_df(Y_train)\n",
    "X_test =  df_test.iloc[:, 1:]\n",
    "Y_test = make_df(Y_test)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_train = X_train.reshape(len(X_train), 3, 3, 28)\n",
    "X_test = X_test.values\n",
    "X_test = X_test.reshape(len(X_test), 3, 3, 28)\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train start --------------------------------------\n",
    "train_start = datetime.datetime.now()\n",
    "# CV start ------------------------------------------------------------\n",
    "for outer_cv in range(outer_cvs):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = data_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True\n",
    "    )\n",
    "    val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "    # Bayesian optimization -------------------------------------\n",
    "    study = optuna.create_study()\n",
    "    try:\n",
    "        study.optimize(opt_cnn, n_trials = n_trials)\n",
    "        # Best_model_training ---------------------------------------\n",
    "        num_filters = [int(study.best_params[f'num_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "        size_filters = [int(study.best_params[f'size_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "        model = create_model(image_shape, int(study.best_params['num_layer']), study.best_params['padding'], int(study.best_params['dense_num']), num_filters, size_filters, study.best_params['dropout_rate_in'], study.best_params['dropout_rate_out'])\n",
    "        sgd = optimizers.SGD(lr = study.best_params['learning_rate'], decay = study.best_params['decay'], momentum = study.best_params['momentum'], nesterov = True, clipvalue = 1.0)\n",
    "        model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            train_datagen.flow(X_outer_train, Y_outer_train, batch_size = 2**int(study.best_params['batch_size']) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_outer_val, Y_outer_val),\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False\n",
    "            )\n",
    "        cv_result_imgs_generator(model, history)\n",
    "        try:         best_params.append(study.best_params)\n",
    "        except:      best_params = [study.best_params]\n",
    "        try:         val_pred_files = np.concatenate((val_pred_files, val_files), axis=0)\n",
    "        except:      val_pred_files = val_files\n",
    "        try:         Y_val_pred = np.concatenate((Y_val_pred, model.predict(X_outer_val).argmax(axis=1)), axis=0)\n",
    "        except:      Y_val_pred = np.array(model.predict(X_outer_val).argmax(axis=1))\n",
    "        try:         Y_val_obs = np.concatenate((Y_val_obs, Y_outer_val), axis=0)\n",
    "        except:      Y_val_obs = Y_outer_val\n",
    "        try:         Y_val_smx = np.concatenate((Y_val_smx, model.predict(X_outer_val)),axis=0)\n",
    "        except:      Y_val_smx = model.predict(X_outer_val)    \n",
    "        print(\"accuracy is\", model.evaluate(X_outer_val, Y_outer_val)[1])\n",
    "        del model\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #compare_TV(history, outer_cv)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "train_end = datetime.datetime.now()\n",
    "spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # train start --------------------------------------\n",
    "# train_start = datetime.datetime.now()\n",
    "# # CV start ------------------------------------------------------------\n",
    "# for outer_cv in range(outer_cvs):\n",
    "#     outer_start = datetime.datetime.now()\n",
    "#     print(f'outer_cv_{outer_cv}_processing....')\n",
    "#     # Data Loader-------------------------------------\n",
    "#     train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = data_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#         horizontal_flip = True,\n",
    "#         vertical_flip = True\n",
    "#     )\n",
    "#     val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "#     # Bayesian optimization -------------------------------------\n",
    "#     study = optuna.create_study()\n",
    "#     try:\n",
    "#         study.optimize(opt_cnn, n_trials = n_trials)\n",
    "#     except:\n",
    "#         pass\n",
    "#     # Best_model_training ---------------------------------------\n",
    "#     num_filters = [int(study.best_params[f'num_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "#     size_filters = [int(study.best_params[f'size_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "#     model = create_model(image_shape, int(study.best_params['num_layer']), study.best_params['padding'], int(study.best_params['dense_num']), num_filters, size_filters, study.best_params['dropout_rate_in'], study.best_params['dropout_rate_out'])\n",
    "#     sgd = optimizers.SGD(lr = study.best_params['learning_rate'], decay = study.best_params['decay'], momentum = study.best_params['momentum'], nesterov = True, clipvalue = 1.0)\n",
    "#     model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     history = model.fit(\n",
    "#         train_datagen.flow(X_outer_train, Y_outer_train, batch_size = 2**int(study.best_params['batch_size']) * gpus),\n",
    "#         epochs = train_epochs,\n",
    "#         validation_data = (X_outer_val, Y_outer_val),\n",
    "#         shuffle = True,\n",
    "#         verbose = 0,\n",
    "#         use_multiprocessing = False\n",
    "#         )\n",
    "#     try:         best_params.append(study.best_params)\n",
    "#     except:      best_params = [study.best_params]\n",
    "#     try:         val_pred_files = np.concatenate((val_pred_files, val_files), axis=0)\n",
    "#     except:      val_pred_files = val_files\n",
    "#     try:         Y_val_pred = np.concatenate((Y_val_pred, model.predict(X_outer_val).argmax(axis=1)), axis=0)\n",
    "#     except:      Y_val_pred = np.array(model.predict(X_outer_val).argmax(axis=1))\n",
    "#     try:         Y_val_obs = np.concatenate((Y_val_obs, Y_outer_val), axis=0)\n",
    "#     except:      Y_val_obs = Y_outer_val\n",
    "#     try:         Y_val_smx = np.concatenate((Y_val_smx, model.predict(X_outer_val)),axis=0)\n",
    "#     except:      Y_val_smx = model.predict(X_outer_val)\n",
    "    \n",
    "#     cv_result_imgs_generator(model, history)\n",
    "#     print(\"accuracy is\", model.evaluate(X_outer_val, Y_outer_val)[1])\n",
    "#     #compare_TV(history, outer_cv)\n",
    "#     del model\n",
    "#     keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     outer_end = datetime.datetime.now()\n",
    "#     spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "#     pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "# train_end = datetime.datetime.now()\n",
    "# spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "# pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save CV_Result -------------------------------------------------\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx)\n",
    "param_names = best_params[list(map(len, best_params)).index(max(list(map(len, best_params))))].keys()\n",
    "best_params_dict = mean_params_calc(param_names)\n",
    "pkl_saver(best_params, 'best_params_list.binaryfile')\n",
    "pkl_saver(best_params_dict, 'best_params.binaryfile')\n",
    "best_params_dict = pkl_loader('best_params.binaryfile')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "results = [val_pred_files, Y_val_obs, Y_val_pred, Y_val_smx]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "results_csv = np.concatenate([pd.DataFrame(val_pred_files),pd.DataFrame(Y_val_obs), pd.DataFrame(Y_val_pred), pd.DataFrame(Y_val_smx)], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_val_obs.astype(int), Y_val_pred)\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "# Int parameter\n",
    "num_layer = int(best_params_dict['num_layer'])\n",
    "num_filters = [int(best_params_dict['num_filter_' + str(i)]) for i in range(num_layer)]\n",
    "size_filters = [int(best_params_dict['size_filter_' + str(i)]) for i in range(num_layer)]\n",
    "dense_num = int(best_params_dict['dense_num'])\n",
    "batch_size = int(best_params_dict['batch_size'])\n",
    "# Uniform parameter\n",
    "# Loguniform parameter\n",
    "lr = best_params_dict['learning_rate']\n",
    "decay = best_params_dict['decay']\n",
    "# Discrete-uniform parameter\n",
    "dropout_rate_in = best_params_dict['dropout_rate_in']\n",
    "dropout_rate_out = best_params_dict['dropout_rate_out']\n",
    "momentum = best_params_dict['momentum']\n",
    "# Categorical parameter\n",
    "padding = best_params_dict['padding']\n",
    "\n",
    "\n",
    "# Model Checkpoint ------------------\n",
    "cp_cb = ModelCheckpoint(\n",
    "    './weights/best_weights.hdf5',\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    mode = 'auto')\n",
    "# Logging ----------------------------------------\n",
    "log_dir = os.path.join('./logs/')\n",
    "tb_cb = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', patience = int(best_epochs/10), verbose = 1)\n",
    "\n",
    "cbs = [cp_cb, tb_cb, es_cb]\n",
    "# Train Best_Model ----------------------------------\n",
    "# For CPU run ------------------\n",
    "best_model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "\n",
    "best_model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy')\n",
    "hist = best_model.fit(\n",
    "    train_datagen.flow(X_train, Y_train, batch_size = (2**batch_size) * gpus),\n",
    "    epochs = best_epochs,\n",
    "    callbacks = cbs,\n",
    "    shuffle = True,\n",
    "    verbose = 1,\n",
    "    initial_epoch = 0,\n",
    "    use_multiprocessing = False)\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "best_model.save('./model/best_model.hdf5')\n",
    "model_trained = True\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ここだけは手動で指定する or モデル学習後なら触らない\n",
    "if model_trained == False:\n",
    "    print(\"timename を手動で設定しましたか？\")\n",
    "    timename = \"2020_1109_1712\"\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ロードだけでも動くように変数の再定義=============================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "os.chdir(os.path.join(result_path, \"CNN\", timename))\n",
    "# Data Loader ------------------------------\n",
    "best_model = load_model(\"./model/best_model.hdf5\")\n",
    "df = pkl_loader(os.path.join(data_path, f'df_{N}x{N}.pkl'))\n",
    "# Data converter ----------------------------------------------\n",
    "filenames, X, Y, point, region = df[0], df[1], df[2], df[3], df[4]\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "num_category = len(np.unique(Y))\n",
    "# Data splitting ----------------------------------------------\n",
    "X_train_mean_lis = []\n",
    "X_train_std_lis = []\n",
    "X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "\n",
    "for i in range(X_train.shape[3]):\n",
    "    X_train_mean_lis.append(X_train[:,:,:,i].mean())\n",
    "    X_train_std_lis.append(X_train[:,:,:,i].std())\n",
    "    X_train[:,:,:,i] = (X_train[:,:,:,i] - X_train_mean_lis[i])/X_train_std_lis[i]\n",
    "    X_test[:,:,:,i] = (X_test[:,:,:,i] - X_train_mean_lis[i])/X_train_std_lis[i]\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "Y_test_pred = [ np.array(best_model.predict(X_test).argmax(axis=1))]\n",
    "np.savetxt('y_test_pred.txt', Y_test_pred)\n",
    "with open(\"best_model_summary.txt\", \"w\") as fp:\n",
    "    best_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "\n",
    "    \n",
    "results = [Y_files, Y_test, Y_test_pred]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "results_csv = np.concatenate([pd.DataFrame(Y_files),pd.DataFrame(Y_test), pd.DataFrame(Y_test_pred[0])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\"]\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int), Y_test_pred[0])\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "test_smr = classification_report(list(np.array(Y_test).astype(int)), list(Y_test_pred[0].astype(int)), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_test.txt','w') as f:\n",
    "    f.write(test_smr)\n",
    "    \n",
    "print('finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 正答率表示\n",
    "(np.array(Y_test_pred).astype(int) == Y_test.astype(int)).sum() / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (1x1 pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "##### Data Loader ------------------------------\n",
    "\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "\n",
    "testfiles.sort()\n",
    "N=1\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "N=3#     中心の一点だけ使います\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_files, X_train, Y_train, train_point, region_train = data_import()\n",
    "\n",
    "# # データの標準化\n",
    "for i in range(7):\n",
    "    standard_list = []\n",
    "    for j in range(len(X_train)):\n",
    "        standard_list.append(X_train[j][1][1][i::7])\n",
    "    print(np.mean(standard_list))\n",
    "    print(np.std(standard_list))\n",
    "    for j in tqdm(range(len(X_train))):\n",
    "        X_train[j][1][1][i::7] = (X_train[j][1][1][i::7] - np.mean(standard_list)) / np.std(standard_list)\n",
    "X_train[0][1][1][0::7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(data_path + f'df_{N}x{N}_standarization.pkl'):\n",
    "    df_XY = pkl_loader(data_path + f'df_{N}x{N}_standarization.pkl')\n",
    "else:\n",
    "    data_num = int(len(imgfiles)/28)\n",
    "    df_XY = pd.DataFrame(np.zeros((data_num,29)))\n",
    "    for i in range(data_num):\n",
    "        df_XY.iloc[i,0] = Y[i]\n",
    "        for j in range(28):\n",
    "            df_XY.iloc[i, j+1] = X[i][1][1][j]\n",
    "    df = [filenames, X, Y, point, region]\n",
    "    pkl_saver(df_XY, os.path.join(data_path, f'df_{N}x{N}_standarization.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     28,
     30,
     48,
     50
    ]
   },
   "outputs": [],
   "source": [
    "##### Data Loader ------------------------------\n",
    "\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "\n",
    "testfiles = glob(f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}_test\" + \"/*.tif\")\n",
    "\n",
    "testfiles.sort()\n",
    "N=1\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# data import ---------------------------------------------------------------------------\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, lgb_boosting_type, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "\n",
    "N=3#     中心の一点だけ使います\n",
    "X_files, X_train, Y_train, train_point, region_train = data_import()\n",
    "N=1\n",
    "\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_standarization.pkl'):\n",
    "    df_XY = pkl_loader(data_path + f'df_{N}x{N}_standarization.pkl')\n",
    "else:\n",
    "    for i in range(7):\n",
    "    standard_list = []\n",
    "    for j in range(len(X_train)):\n",
    "        standard_list.append(X_train[j][1][1][i::7])\n",
    "    for j in tqdm(range(len(X_train))):\n",
    "        X_train[j][1][1][i::7] = (X_train[j][1][1][i::7] - np.mean(standard_list)) / np.std(standard_list)\n",
    "        \n",
    "    data_num = int(len(imgfiles)/28)\n",
    "    df_XY = pd.DataFrame(np.zeros((data_num,29)))\n",
    "    for i in range(data_num):\n",
    "        df_XY.iloc[i,0] = Y[i]\n",
    "        for j in range(28):\n",
    "            df_XY.iloc[i, j+1] = X[i][1][1][j]\n",
    "    pkl_saver(df_XY, os.path.join(data_path, f'df_{N}x{N}_standarization.pkl'))\n",
    "    \n",
    "\n",
    "Y_files, X_test, Y_test, test_point, region_test = test_import()\n",
    "if os.path.exists(data_path + f'df_{N}x{N}_test_standarization.pkl'):\n",
    "    df_test = pkl_loader(data_path + f'df_{N}x{N}_test_standarization.pkl')\n",
    "else:\n",
    "    for i in range(7):\n",
    "    standard_list = []\n",
    "    for j in range(len(X_test)):\n",
    "        standard_list.append(X_test[j][1][1][i::7])\n",
    "    for j in tqdm(range(len(X_test))):\n",
    "        X_test[j][1][1][i::7] = (X_test[j][1][1][i::7] - np.mean(standard_list)) / np.std(standard_list)\n",
    "        \n",
    "    data_num = int(len(testfiles)/28)\n",
    "    df_test = pd.DataFrame(np.zeros((data_num,29)))\n",
    "    for i in range(data_num):\n",
    "        df_test.iloc[i,0] = Y[i]\n",
    "        for j in range(28):\n",
    "            df_test.iloc[i, j+1] = X[i][1][1][j]\n",
    "    pkl_saver(df_test, os.path.join(data_path, f'df_{N}x{N}_test_standarization.pkl'))\n",
    "        \n",
    "# Data converter ----------------------------------------------\n",
    "Y_train, X_train = df_XY[0], df_XY.iloc[:, 1:]\n",
    "Y_test, X_test = df_test[0], df_test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i in range(100):\n",
    "    num += (X[i][1][1] == 0).sum()==28\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_list = []\n",
    "for i in range(4):\n",
    "    X_train.loc[:,i::4] = X_train.loc[:,i::4] / X_train.loc[:,i::4].mean()\n",
    "X_train.loc[0,0::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train start -----------------------------------------------------------------------------\n",
    "train_start = datetime.datetime.now()\n",
    "\n",
    "for outer_cv in range(outer_cvs):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_    \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(opt_lgb, n_trials=n_trials)\n",
    "#     print(study.best_value)\n",
    "    \n",
    "    lgb_best_param = study.best_params\n",
    "    lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "    lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "    \n",
    "    print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val) ) )\n",
    "    print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test).astype(int)      == Y_test.astype(int).values).sum() / len(Y_test) ) )\n",
    "    #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "    \n",
    "    try:\n",
    "        Y_val_files.append(val_files)\n",
    "    except:\n",
    "        Y_val_files =  [val_files]    \n",
    "    try:\n",
    "        lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val))\n",
    "    except:\n",
    "        lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val)]\n",
    "    try:\n",
    "        lgb_best_params.append(lgb_best_param)\n",
    "    except:\n",
    "        lgb_best_params = [lgb_best_param]\n",
    "    try:\n",
    "        Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "    except:\n",
    "        Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "    try:\n",
    "        Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "    except:\n",
    "        Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "    try:\n",
    "        Y_val_obs.append(Y_outer_val[0].values.astype(int))\n",
    "    except:\n",
    "        Y_val_obs =  [Y_outer_val[0].values.astype(int)]\n",
    "\n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))\n",
    "\n",
    "# Y_val_pred = [Y_val_pred[i:i+5454] for i in range(10)]\n",
    "# Y_val_smx = [Y_val_smx[i:i+5454] for i in range(10)]\n",
    "train_end = datetime.datetime.now()\n",
    "spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "pkl_saver(spend_time, os.path.join(time_path, \"all_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "make_df(val_files)\n",
    "results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_val_obs[best_trial_num].astype(int), Y_val_pred[best_trial_num])\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int).values, best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "# best_model.save('./model/best_model.hdf5')\n",
    "# model_trained = True\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Show Accuracy --------------------------------------------------------------------------\n",
    "np.array(best_model.predict(X_test).astype(int) == Y_test.astype(int).values).sum() / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (3x3 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data Loader ------------------------------\n",
    "if under==20:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_old/{N}x{N}\"\n",
    "elif under==90:\n",
    "    train_tif_name = f\"D:/LULC/features/01_landsat8/train_new/{N}x{N}\"\n",
    "root_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/\"\n",
    "result_path    = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/{N}x{N}\"\n",
    "data_path      = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/data/\"\n",
    "model_path     = f\"C:/Users/GE/Dropbox/Kairo/under{under}_results/model/{N}x{N}/\"\n",
    "\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "imgfiles.sort()\n",
    "model_trained = False\n",
    "\n",
    "# Data converter ----------------------------------------------\n",
    "# X->説明変数, Y->目的変数, point->緯度経度, region->領域を10分割した時の分割区間名\n",
    "filenames, X, Y, point, region = data_import()\n",
    "X = X.astype(np.float64)\n",
    "Y = Y.astype(np.float64)\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "num_category = len(np.unique(Y))\n",
    "\n",
    "# Data standardizing ----------------------------------------------\n",
    "X_train_mean_lis, X_train_std_lis = [], []\n",
    "X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# pre processing ------------------------------\n",
    "for i in range(X_train.shape[3]):\n",
    "    X_train_mean_lis.append(X_train[:,:,:,i].mean())\n",
    "    X_train_std_lis.append(X_train[:,:,:,i].std())\n",
    "    X_train[:,:,:,i] = (X_train[:,:,:,i] - X_train_mean_lis[i]) / X_train_std_lis[i]\n",
    "    X_test[:,:,:,i] = (X_test[:,:,:,i] - X_train_mean_lis[i]) / X_train_std_lis[i]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], N*N*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], N*N*28)\n",
    "timename       = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path      =  os.path.join(result_path, timename, \"outer_cv_times\")\n",
    "make_dirs(lgb_boosting_type)\n",
    "\n",
    "# Saving region_separate_map\n",
    "region_image_generator(point, region)\n",
    "time_path += f\"_{lgb_boosting_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train start -----------------------------------------------------------------------------\n",
    "train_start = datetime.datetime.now()\n",
    "\n",
    "for outer_cv in range(outer_cvs):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = lgb_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    val_train_region = KMeans(n_clusters = outer_cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "\n",
    "    inner_cvs = 1\n",
    "    \n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(opt_lgb, n_trials=n_trials)\n",
    "#     print(study.best_value)\n",
    "    \n",
    "    lgb_best_param = study.best_params\n",
    "    lgb_best = LGBMClassifier(**lgb_best_param)\n",
    "    lgb_best.fit(X_outer_train, Y_outer_train)\n",
    "    \n",
    "    print('mean of outer_val_scores is ', (np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val) ) )\n",
    "    print('mean of test_scores is ',      (np.array(lgb_best.predict(X_test).astype(int)      == Y_test.astype(int)).sum() / len(Y_test) ) )\n",
    "    #     print(lgb_best.predict_proba(X_outer_val).argmax(axis=1))\n",
    "    \n",
    "    try:\n",
    "        Y_val_files.append(val_files)\n",
    "    except:\n",
    "        Y_val_files =  [val_files]    \n",
    "    try:\n",
    "        lgb_scores.append(np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val))\n",
    "    except:\n",
    "        lgb_scores = [np.array(lgb_best.predict(X_outer_val).astype(int) == Y_outer_val.astype(int)[0].values).sum() / len(Y_outer_val)]\n",
    "    try:\n",
    "        lgb_best_params.append(lgb_best_param)\n",
    "    except:\n",
    "        lgb_best_params = [lgb_best_param]\n",
    "    try:\n",
    "        Y_val_smx.append(np.array(lgb_best.predict_proba(X_outer_val)))\n",
    "    except:\n",
    "        Y_val_smx = [np.array(lgb_best.predict_proba(X_outer_val))]\n",
    "    try:\n",
    "        Y_val_pred.append(lgb_best.predict(X_outer_val).astype(int))\n",
    "    except:\n",
    "        Y_val_pred = [lgb_best.predict(X_outer_val).astype(int)]\n",
    "    try:\n",
    "        Y_val_obs.append(Y_outer_val[0].values.astype(int))\n",
    "    except:\n",
    "        Y_val_obs =  [Y_outer_val[0].values.astype(int)]\n",
    "\n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, f\"./outer_cv_times/outer_cv_{outer_cv}_time.txt\")\n",
    "\n",
    "# Y_val_pred = [Y_val_pred[i:i+5454] for i in range(10)]\n",
    "# Y_val_smx = [Y_val_smx[i:i+5454] for i in range(10)]\n",
    "train_end = datetime.datetime.now()\n",
    "spend_time = f\"Outer_cv time is {train_end - train_start} seconds.\"\n",
    "pkl_saver(spend_time, \"./outer_cv_times/all_time.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save Results -------------------------------------------------------------------------\n",
    "best_trial_num = np.argmax(lgb_scores)\n",
    "\n",
    "np.savetxt('Y_val_smx.txt', Y_val_smx[:][best_trial_num])\n",
    "param_names = lgb_best_params[list(map(len, lgb_best_params)).index(max(list(map(len, lgb_best_params))))].keys()\n",
    "best_params_dict = lgb_best_params[best_trial_num]\n",
    "pkl_saver(lgb_best_params, 'best_params_list.csv')\n",
    "pkl_saver(lgb_best_params, 'best_params.csv')\n",
    "best_params_dict = pkl_loader('best_params.csv')\n",
    "\n",
    "# Save CV_Result to csv -------------------------------------------------\n",
    "\n",
    "results = [Y_val_files[best_trial_num], Y_val_obs[best_trial_num], Y_val_pred[best_trial_num], Y_val_smx[:][best_trial_num]]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "make_df(val_files)\n",
    "results_csv = np.concatenate([make_df(Y_val_files[best_trial_num]),make_df(Y_val_obs[best_trial_num]), make_df(Y_val_pred[best_trial_num]), make_df(Y_val_smx[:][best_trial_num])], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_val.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_val_obs[best_trial_num].astype(int), Y_val_pred[best_trial_num])\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_val.csv\")\n",
    "\n",
    "res_smr = classification_report(list(results_csv['obs'].astype(int)), list(results_csv['pred']), target_names = labels, labels = np.array(range(len(labels))))\n",
    "with open('./results/result_summary_val.txt','w') as f:\n",
    "    f.write(res_smr)\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "best_model = LGBMClassifier(**lgb_best_params[best_trial_num])\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "columns = [\"name\", \"obs\", \"pred\", 'Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results_test.csv')\n",
    "labels = ['Water', 'Urban and built-up', 'Rice paddy',  'Crops', 'Grassland', 'DBF', 'DNF', 'EBF', 'ENF', 'Bare land']\n",
    "cf_metr = confusion_matrix(Y_test.astype(int), best_model.predict(X_test).astype(int))\n",
    "cf_metr = pd.DataFrame(cf_metr)\n",
    "cf_metr.columns=labels\n",
    "cf_metr.index=labels\n",
    "cf_metr.to_csv(\"./results/confusion_matrix_test.csv\")\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "# best_model.save('./model/best_model.hdf5')\n",
    "# model_trained = True\n",
    "\n",
    "# Save Code\n",
    "import shutil\n",
    "os.mkdir(\"./code\")\n",
    "code_name = ipynb_path.get().split(\"/\")[-1]\n",
    "shutil.copy(ipynb_path.get(), f\"./code/{code_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y_val_files, lgb_scores, lgb_best_params,  Y_val_smx, Y_val_pred, Y_val_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
