{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(2, 3), b.shape=(3, 2), m=2, n=2, k=3 [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-01a1aec7aa79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ge\\anaconda3\\envs\\lulc\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ge\\anaconda3\\envs\\lulc\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2982\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2984\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ge\\anaconda3\\envs\\lulc\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5575\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5576\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5577\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5578\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5579\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ge\\anaconda3\\envs\\lulc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6652\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6653\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6654\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ge\\anaconda3\\envs\\lulc\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(2, 3), b.shape=(3, 2), m=2, n=2, k=3 [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# Utils -----------------------\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, zipfile, io, re\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "# Machine Learning ---------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import sqrt\n",
    "import optuna\n",
    "from optuna import integration\n",
    "# Keras, TensorFlow ---------------\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization, Convolution2D, Input\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tifffile\n",
    "\n",
    "\n",
    "# Specify a device\n",
    "with tf.device('/device:GPU:1'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "print(c)\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "gpus = 2\n",
    "SEED = 31\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Functions ------------------------------\n",
    "def pkl_saver(object, pkl_filename):\n",
    "    with open(pkl_filename, 'wb') as web:\n",
    "        pickle.dump(object , web)\n",
    "\n",
    "\n",
    "def pkl_loader(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as web:\n",
    "        data = pickle.load(web)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Dir generator ----------------------------\n",
    "def dir_generator(dir_path):\n",
    "    if os.path.exists(dir_path) == False:\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "# Data Loader ----------------------------------\n",
    "def crown_DataLoader(zip_name):\n",
    "    z = zipfile.ZipFile(zip_name)\n",
    "    imgfiles = [x for x in z.namelist()]\n",
    "    #imgfiles = [x for x in z.namelist() if re.search(r'^' + zip_name.split('.')[0] + '.tif$', x)]\n",
    "    filenames = []\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    point = []\n",
    "    max_light = 0\n",
    "    print('NTL_processing...')\n",
    "    ext = ('.tif')\n",
    "    for imgfile in tqdm(imgfiles):\n",
    "        if imgfile.endswith(ext):\n",
    "            print(imgfile)\n",
    "            image = Image.open(io.BytesIO(z.read(imgfile)))\n",
    "            data = np.asarray(image).reshape(image_size,image_size,-1)\n",
    "            file = os.path.basename(imgfile)\n",
    "            file_split = [i for i in file.split('_')]\n",
    "            y = float(os.path.splitext(file_split[3])[0])\n",
    "            filenames.append(file)\n",
    "            X.append(data)\n",
    "            Y.append(y)\n",
    "            point.append([float(file_split[1]), float(file_split[2])])\n",
    "    z.close()\n",
    "    filenames = np.array(filenames)\n",
    "    X = np.array(X).astype('float32')\n",
    "    Y = np.array(Y).astype('float32')\n",
    "    print(X.shape, Y.shape)\n",
    "    return filenames, X, Y, point\n",
    "\n",
    "\n",
    "def region_visualizer(df):\n",
    "    points = df[3]\n",
    "    lon, lat = [], []\n",
    "    for point in points:\n",
    "        lon.append(point[0])\n",
    "        lat.append(point[1])\n",
    "    lon = np.array(lon).astype(float).reshape(-1,1)\n",
    "    lat = np.array(lat).astype(float).reshape(-1,1)\n",
    "    region = df[4]\n",
    "    region = np.array(region).astype(int).reshape(-1,1)\n",
    "    df = pd.DataFrame(np.concatenate([lon, lat, region], axis=1))\n",
    "    df.columns = ['longitude', 'latitude', 'region_class']\n",
    "    pivotted = df.pivot('longitude', 'latitude', 'region_class')\n",
    "    for i in range(pivotted.shape[0]):\n",
    "        pivotted.iloc[i] = pd.to_numeric(pivotted.iloc[i])\n",
    "    pivotted.columns = pd.to_numeric(pivotted.columns)\n",
    "    pivotted.index = pd.to_numeric(pivotted.index)\n",
    "    pivotted = pivotted.fillna(-1)\n",
    "    pivotted = pivotted.astype(float).T\n",
    "    cmap = sns.color_palette(\"deep\", cvs + 1)\n",
    "    cmap[0] = (0,0,0)\n",
    "    plt = sns.heatmap(pivotted, cmap = cmap)\n",
    "    plt.invert_yaxis()\n",
    "    colorbar = plt.collections[0].colorbar\n",
    "    r = colorbar.vmax - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + 0.5 * r / (cvs + 1) + r * i / (cvs + 1) for i in range(cvs + 1)])\n",
    "    colorbar.set_ticklabels(['background']+list(range(cvs)))\n",
    "    plt.figure.savefig(os.path.join(result_path, \"result_img\", \"region_map.jpg\"))\n",
    "    del(plt)\n",
    "\n",
    "\n",
    "def data_splitter_cv(filenames, X, Y, cv, region, point):\n",
    "    test_index = np.where(region==cv)\n",
    "    train_index = np.setdiff1d(np.arange(0, X.shape[0], 1), test_index)\n",
    "    train_files = filenames[train_index]\n",
    "    test_files = filenames[test_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_test = Y[test_index]\n",
    "    X_train = X[train_index]\n",
    "    Y_train = Y[train_index]\n",
    "    train_region = region[train_index]\n",
    "    train_point = point[train_index]\n",
    "    return train_files, test_files, X_train, X_test, Y_train, Y_test, train_region, train_point\n",
    "\n",
    "\n",
    "# Loss Definition ----------------------------------\n",
    "def root_mean_squared_error(Y_true, Y_pred):\n",
    "    return K.sqrt(K.mean(K.square(Y_pred - Y_true), axis = -1))\n",
    "\n",
    "\n",
    "def create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out):\n",
    "    inputs = Input(image_shape)\n",
    "    for d in ['/gpu:0', '/gpu:1']:\n",
    "        with tf.device(d):\n",
    "            x = Dropout(dropout_rate_in)(inputs)\n",
    "            x = Convolution2D(filters = 2**num_filters[0], kernel_size = (size_filters[0],size_filters[0]), padding = 'same', activation = 'relu')(x)\n",
    "            for i in range(1, num_layer):\n",
    "                x = Convolution2D(filters = 2**num_filters[i],\n",
    "                                  kernel_size = (size_filters[i], size_filters[i]),\n",
    "                                  padding = padding,\n",
    "                                  activation = 'relu')(x)\n",
    "                x = MaxPooling2D()(x)\n",
    "            x = Convolution2D(filters = 7,\n",
    "                              kernel_size = (3, 3),\n",
    "                              padding = 'same',\n",
    "                              activation = 'relu')(x)\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(units = 2**dense_num, activation = 'relu')(x)\n",
    "            x = Dropout(dropout_rate_out)(x)\n",
    "            x = Dense(units = num_category, activation = 'softmax')(x)\n",
    "            model = Model(inputs = inputs, outputs = x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def opt_cnn(trial):\n",
    "    # Opt params -----------------------\n",
    "    # Categorical parameter\n",
    "    num_layer = trial.suggest_int('num_layer', 1, 2)\n",
    "    dense_num = trial.suggest_int('dense_num', 2, 5)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(f'num_filter_{i}', 2, 5, 1)) for i in range(num_layer)]\n",
    "    size_filters = [int(trial.suggest_discrete_uniform(f'size_filter_{i}', 3, 5, 2)) for i in range(num_layer)]\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 5)\n",
    "    # Model Compiler -----------------------\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    decay = trial.suggest_loguniform('decay', 1e-6, 1e-3)\n",
    "    # Discrete-uniform parameter\n",
    "    dropout_rate_in = trial.suggest_discrete_uniform('dropout_rate_in', 0.0, 0.5, 0.1)\n",
    "    dropout_rate_out = trial.suggest_discrete_uniform('dropout_rate_out', 0.0, 0.5, 0.1)\n",
    "    momentum = trial.suggest_discrete_uniform('momentum', 0.0, 1.0, 0.1)\n",
    "    # categorical parameter\n",
    "#    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"rmsprop\", \"adam\"])\n",
    "    padding = trial.suggest_categorical('padding', ['same', 'valid'])\n",
    "    # compile model-------------------\n",
    "    model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True)\n",
    "#    sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "    # For CPU run ------------------\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Train Model ----------------------------------\n",
    "    es_cb = EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 0)\n",
    "    pr_cb = integration.TFKerasPruningCallback(trial, 'val_loss')\n",
    "    cbs = [es_cb, pr_cb]\n",
    "    loss_list, acc_list = [], []\n",
    "    for inner_cv in range(0, cvs):\n",
    "        _, _, X_inner_train, X_inner_val, Y_inner_train, Y_inner_val, _, _ = data_splitter_cv(train_files, X_outer_train, Y_outer_train, inner_cv, val_train_region, val_train_point)\n",
    "        hist = model.fit(\n",
    "            train_datagen.flow(X_inner_train, Y_inner_train, batch_size = (2**batch_size) * gpus),\n",
    "            epochs = train_epochs,\n",
    "            validation_data = (X_inner_val, Y_inner_val),\n",
    "            callbacks = cbs,\n",
    "            shuffle = True,\n",
    "            verbose = 0,\n",
    "            use_multiprocessing = False)\n",
    "        loss_list += [model.evaluate(X_inner_val, Y_inner_val)[0]]\n",
    "        acc_list += [model.evaluate(X_inner_val, Y_inner_val)[1]]\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    eval_loss = np.mean(loss_list)\n",
    "    eval_acc = np.mean(acc_list)\n",
    "    return (1 - eval_acc)\n",
    "\n",
    "\n",
    "def mean_params_calc(param_names):\n",
    "    dict = {}\n",
    "    categoricals = ['padding']\n",
    "    for param_name in param_names:\n",
    "        data_num = 0\n",
    "        if param_name not in categoricals:\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    try:\n",
    "                        dict[param_name] += data[param_name]\n",
    "                    except:\n",
    "                        dict[param_name] = data[param_name]\n",
    "                    data_num = data_num + 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = dict[param_name]/data_num\n",
    "        else:\n",
    "            categorical_list = []\n",
    "            for data in best_params:\n",
    "                try:\n",
    "                    categorical_list = categorical_list + [data[param_name]]\n",
    "                except:\n",
    "                    pass\n",
    "            dict[param_name] = stats.mode(categorical_list)[0][0]\n",
    "    return dict\n",
    "\n",
    "\n",
    "def cv_result_imgs_generator(model, history):\n",
    "    # Visualize Loss Results ----------------------------\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\", marker=\"o\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\", marker=\"o\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(color='gray', alpha=0.2)\n",
    "    plt.savefig('./img_loss/' + str(outer_cv) + '_loss.jpg')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generalization_result_imgs_generator(name, Y_val_pred, Y_val_all):\n",
    "    # Evaluate test data -----------------------\n",
    "    plt.figure()\n",
    "    plt.scatter(Y_val_all, Y_val_pred, s=3, alpha=0.5)\n",
    "    plt.xlim(min([np.min(Y_val_all), np.min(Y_val_pred)]), max([np.max(Y_val_all),np.max(Y_val_pred)]))\n",
    "    plt.xlabel(\"obs\")\n",
    "    plt.ylabel(\"pred\")\n",
    "    x = np.linspace(min([np.min(Y_val_all), np.min(Y_val_pred)]), max([np.max(Y_val_all),np.max(Y_val_pred)]),100)\n",
    "    y = x\n",
    "    plt.plot(x, y, \"r-\")\n",
    "    plt.savefig('./img_loss/' + name + '_scatter_test.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 11\n",
    "image_size = N\n",
    "\n",
    "# Data Loader ------------------------------\n",
    "train_tif_name = f\"C:/Users/GE/Desktop/kairo_something/train/{N}*{N}\"\n",
    "root_path = \"C:/Users/GE/Dropbox/Kairo/\"\n",
    "result_path = f\"C:/Users/GE/Dropbox/Kairo/test_{N}x{N}_results\"\n",
    "data_path = \"C:/Users/GE/Dropbox/Kairo/data/df_old/\"\n",
    "timename = '{0:%Y_%m%d_%H%M}'.format(datetime.datetime.now())\n",
    "time_path =  os.path.join(result_path, timename, \"outer_cv_times\")\n",
    "\n",
    "# dir generation\n",
    "dir_generator(result_path)\n",
    "# Chenge current directry\n",
    "os.mkdir(os.path.join(result_path, timename))\n",
    "os.chdir(os.path.join(result_path, timename))\n",
    "\n",
    "dir_generator(\"./results/\")\n",
    "dir_generator(\"./img_loss/\")\n",
    "dir_generator(\"./model/\")\n",
    "dir_generator(\"./weights/\")\n",
    "dir_generator(\"./logs/\")\n",
    "dir_generator(\"./outer_cv_times/\")\n",
    "\n",
    "imgfiles = glob(train_tif_name + \"/*.tif\")\n",
    "\n",
    "# Training params ------------------------------------------\n",
    "train_epochs = 2**3\n",
    "ntrials = 2**3\n",
    "cvs = 10\n",
    "best_epochs = 2**5\n",
    "early_stopping = 2**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pkl\n",
    "if os.path.exists(data_path + 'test.pkl'):\n",
    "    df = pkl_loader(data_path + 'test.pkl')\n",
    "else:\n",
    "    X = [] # X: 説明変数 = (N*N)*(7*4)のデータ\n",
    "    Y = [] # Y: 目的変数\n",
    "    point = [] # point: 緯度経度\n",
    "    X_28 = []\n",
    "    Y_28 = 0\n",
    "    point_28 = []\n",
    "    filenames = []\n",
    "    max_light = 0\n",
    "    print('inputdata_processing...')\n",
    "    ext = ('.tif')\n",
    "    trial = 300\n",
    "    for box in tqdm(range(trial)):\n",
    "        for imgfile in imgfiles[box*28: (box+1)*28]:\n",
    "            # ZIPから画像読み込み\n",
    "            image = tifffile.imread(imgfile)\n",
    "    #         print(image.shape)\n",
    "            file = os.path.basename(imgfile)\n",
    "            file_split = [i for i in file.split('_')]\n",
    "            X_28.append(image)\n",
    "        Y_28 = file_split[5].split(\".\")[0]\n",
    "        point_28 = [float(file_split[1]), float(file_split[2])]\n",
    "        filenames.append(f\"{file_split[0]}_{file_split[1]}_{file_split[2]}_{Y_28}\")\n",
    "        X.append(X_28[box*28: (box+1)*28])\n",
    "        Y.append(Y_28)\n",
    "        point.append(point_28)\n",
    "    del X_28, Y_28, point_28\n",
    "    X = np.asarray(X)\n",
    "    print(X.shape)\n",
    "    X = X.transpose(0,2,3,1)\n",
    "    print(X.shape)\n",
    "    Y = np.array(Y)\n",
    "    filenames = np.array(filenames)\n",
    "    point = np.array(point)\n",
    "    region = KMeans(n_clusters = cvs, random_state=SEED).fit(point).labels_\n",
    "    # label encorder===========================================\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(Y)\n",
    "    le_classes = le.classes_\n",
    "    print(le.classes_)\n",
    "    Y = le.transform(Y)\n",
    "    df = [filenames, X, Y, point, region]\n",
    "    pkl_saver(df, os.path.join(data_path, \"test.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data converter ----------------------------------------------\n",
    "# X->説明変数, Y->目的変数, point->緯度経度, region->領域を10分割した時の分割区間名\n",
    "filenames, X, Y, point, region = df[0], df[1], df[2], df[3], df[4]\n",
    "image_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "num_category = len(np.unique(Y))\n",
    "\n",
    "# Data splitting ----------------------------------------------\n",
    "X_files, Y_files, X_train, X_test, Y_train, Y_test, region_train, _, train_point, _ = train_test_split(filenames, X, Y, region, point, test_size=0.2)\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean)/X_train_std\n",
    "X_test = (X_test - X_train_mean)/X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Model ----------------------------------\n",
    "# CV start ------------------------------------------------------------\n",
    "\n",
    "for outer_cv in range(2):\n",
    "    outer_start = datetime.datetime.now()\n",
    "    print(f'outer_cv_{outer_cv}_processing....')\n",
    "    # Data Loader-------------------------------------\n",
    "    train_files, val_files, X_outer_train, X_outer_val, Y_outer_train, Y_outer_val, val_train_region, val_train_point = data_splitter_cv(X_files, X_train, Y_train, outer_cv, region_train, train_point)\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 360,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True\n",
    "    )\n",
    "    val_train_region = KMeans(n_clusters = cvs, random_state=SEED).fit(val_train_point).labels_\n",
    "    # Bayesian optimization -------------------------------------\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(opt_cnn, n_trials = ntrials)\n",
    "    # Best_model_training ---------------------------------------\n",
    "    num_filters = [int(study.best_params[f'num_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "    size_filters = [int(study.best_params[f'size_filter_{i}']) for i in range(int(study.best_params['num_layer']))]\n",
    "    model = create_model(image_shape, int(study.best_params['num_layer']), study.best_params['padding'], int(study.best_params['dense_num']), num_filters, size_filters, study.best_params['dropout_rate_in'], study.best_params['dropout_rate_out'])\n",
    "    sgd = optimizers.SGD(lr = study.best_params['learning_rate'], decay = study.best_params['decay'], momentum = study.best_params['momentum'], nesterov = True, clipvalue = 1.0)\n",
    "    model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(X_outer_train, Y_outer_train, batch_size = 2**int(study.best_params['batch_size']) * gpus),\n",
    "        epochs = train_epochs,\n",
    "        validation_data = (X_outer_val, Y_outer_val),\n",
    "        shuffle = True,\n",
    "        verbose = 0,\n",
    "        use_multiprocessing = False\n",
    "        )\n",
    "    try:\n",
    "        best_params.append(study.best_params)\n",
    "    except:\n",
    "        best_params = study.best_params\n",
    "    try:\n",
    "        val_pred_files = np.concatenate((val_pred_files, val_files), axis=0)\n",
    "    except:\n",
    "        val_pred_files = val_files\n",
    "    try:\n",
    "        Y_val_pred = np.concatenate((Y_val_pred, model.predict(X_outer_val).argmax(axis=1)), axis=0)\n",
    "    except:\n",
    "        Y_val_pred = np.array(model.predict(X_outer_val).argmax(axis=1))\n",
    "    try:\n",
    "        Y_val_obs = np.concatenate((Y_val_obs, Y_outer_val), axis=0)\n",
    "    except:\n",
    "        Y_val_obs = Y_outer_val\n",
    "    try:\n",
    "        Y_val_smx = np.concatenate((Y_val_smx, model.predict(X_outer_val)),axis=0)\n",
    "    except:\n",
    "        Y_val_smx = model.predict(X_outer_val)\n",
    "    cv_result_imgs_generator(model, history)\n",
    "    print(\"accuracy is\", model.evaluate(X_outer_val, Y_outer_val)[1])\n",
    "    #compare_TV(history, outer_cv)\n",
    "    del model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    outer_end = datetime.datetime.now()\n",
    "    spend_time = f\"Outer_cv time is {outer_end - outer_start} seconds.\"\n",
    "    pkl_saver(spend_time, os.path.join(time_path, f\"outer_cv_{outer_cv}_time.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CV_Result -------------------------------------------------\n",
    "results = [val_pred_files, Y_val_obs, Y_val_pred, Y_val_smx]\n",
    "pkl_saver(results, './results/results.pkl')\n",
    "results_csv = np.concatenate([pd.DataFrame(val_pred_files),pd.DataFrame(Y_val_obs), pd.DataFrame(Y_val_pred), pd.DataFrame(Y_val_smx)], 1)\n",
    "results_csv = pd.DataFrame(results_csv)\n",
    "columns = [\"name\", \"pred\", \"obs\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "results_csv.columns=columns\n",
    "results_csv.to_csv('./results/results.csv')\n",
    "pkl_saver(best_params, './results/best_params_list.binaryfile')\n",
    "\n",
    "# Best Model Training -----------------------------------------------\n",
    "generalization_result_imgs_generator('val', Y_val_pred, Y_val_obs)\n",
    "np.savetxt('./results/Y_val_smx.txt', Y_val_smx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[key for key in best_params.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [key for key in best_params.keys()]\n",
    "best_params_dict = mean_params_calc(param_names)\n",
    "pkl_saver(best_params, './results/best_params_list.binaryfile')\n",
    "pkl_saver(best_params_dict, './results/best_params.binaryfile')\n",
    "best_params_dict = pkl_loader('./results/best_params.binaryfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = best_params[key for key in  best_params.keys()]\n",
    "best_params_dict = mean_params_calc(param_names)\n",
    "# pkl_saver(best_params, './results/best_params_list.binaryfile')\n",
    "# pkl_saver(best_params_dict, './results/best_params.binaryfile')\n",
    "# best_params_dict = pkl_loader('./results/best_params.binaryfile')\n",
    "# Int parameter\n",
    "num_layer = int(best_params_dict['num_layer'])\n",
    "num_filters = [int(best_params_dict[f'num_filter_{i}']) for i in range(num_layer)]\n",
    "size_filters = [int(best_params_dict[f'size_filter_{i}']) for i in range(num_layer)]\n",
    "dense_num = int(best_params_dict['dense_num'])\n",
    "batch_size = int(best_params_dict['batch_size'])\n",
    "lr = best_params_dict['learning_rate']\n",
    "decay = best_params_dict['decay']\n",
    "dropout_rate_in = best_params_dict['dropout_rate_in']\n",
    "dropout_rate_out = best_params_dict['dropout_rate_out']\n",
    "momentum = best_params_dict['momentum']\n",
    "padding = best_params_dict['padding']\n",
    "\n",
    "\n",
    "# Model Checkpoint ------------------\n",
    "cp_cb = ModelCheckpoint(\n",
    "    './weights/best_weights.hdf5',\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 0,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True,\n",
    "    mode = 'auto')\n",
    "# Logging ----------------------------------------\n",
    "log_dir = os.path.join('./logs/')\n",
    "tb_cb = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "es_cb = EarlyStopping(monitor = 'val_loss', patience = int(best_epochs/10), verbose = 0)\n",
    "cbs = [cp_cb, tb_cb, es_cb]\n",
    "\n",
    "# Train Best_Model ----------------------------------\n",
    "# For CPU run ------------------\n",
    "best_model = create_model(image_shape, num_layer, padding, dense_num, num_filters, size_filters, dropout_rate_in, dropout_rate_out)\n",
    "sgd = optimizers.SGD(lr = lr, decay = decay, momentum = momentum, nesterov = True, clipvalue = 1.0)\n",
    "\n",
    "print(\"best_model is fitting...\")\n",
    "best_model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy')\n",
    "hist = best_model.fit(\n",
    "    train_datagen.flow(X_train, Y_train, batch_size = (2**batch_size) * gpus),\n",
    "    epochs = best_epochs,\n",
    "    callbacks = cbs,\n",
    "    shuffle = True,\n",
    "    verbose = 0,\n",
    "    initial_epoch = 0,\n",
    "    use_multiprocessing = False)\n",
    "\n",
    "# Save Model -----------------------------------\n",
    "# best_model.save(save_path + 'model/best_model.hdf5')\n",
    "\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(os.path.join(root_path, 'code/3_study_code/LULC_code/LULC_CNN_model-light.ipynb'), './current_code.ipynb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
